<div aria-labelledby="firstHeading" class="vector-body" data-mw-ve-target-container="" id="bodyContent">
 <div class="vector-body-before-content">
  <div class="mw-indicators">
  </div>
  <div class="noprint" id="siteSub">
   From Wikipedia, the free encyclopedia
  </div>
 </div>
 <div id="contentSub">
  <div id="mw-content-subtitle">
  </div>
 </div>
 <div class="mw-body-content" id="mw-content-text">
  <div class="mw-content-ltr mw-parser-output" dir="ltr" lang="en">
   <div class="shortdescription nomobile noexcerpt noprint searchaux" style="display:none">
    Propensity for humans to favor suggestions from automated decision-making systems
   </div>
   <style data-mw-deduplicate="TemplateStyles:r1129693374">
    .mw-parser-output .hlist dl,.mw-parser-output .hlist ol,.mw-parser-output .hlist ul{margin:0;padding:0}.mw-parser-output .hlist dd,.mw-parser-output .hlist dt,.mw-parser-output .hlist li{margin:0;display:inline}.mw-parser-output .hlist.inline,.mw-parser-output .hlist.inline dl,.mw-parser-output .hlist.inline ol,.mw-parser-output .hlist.inline ul,.mw-parser-output .hlist dl dl,.mw-parser-output .hlist dl ol,.mw-parser-output .hlist dl ul,.mw-parser-output .hlist ol dl,.mw-parser-output .hlist ol ol,.mw-parser-output .hlist ol ul,.mw-parser-output .hlist ul dl,.mw-parser-output .hlist ul ol,.mw-parser-output .hlist ul ul{display:inline}.mw-parser-output .hlist .mw-empty-li{display:none}.mw-parser-output .hlist dt::after{content:": "}.mw-parser-output .hlist dd::after,.mw-parser-output .hlist li::after{content:" · ";font-weight:bold}.mw-parser-output .hlist dd:last-child::after,.mw-parser-output .hlist dt:last-child::after,.mw-parser-output .hlist li:last-child::after{content:none}.mw-parser-output .hlist dd dd:first-child::before,.mw-parser-output .hlist dd dt:first-child::before,.mw-parser-output .hlist dd li:first-child::before,.mw-parser-output .hlist dt dd:first-child::before,.mw-parser-output .hlist dt dt:first-child::before,.mw-parser-output .hlist dt li:first-child::before,.mw-parser-output .hlist li dd:first-child::before,.mw-parser-output .hlist li dt:first-child::before,.mw-parser-output .hlist li li:first-child::before{content:" (";font-weight:normal}.mw-parser-output .hlist dd dd:last-child::after,.mw-parser-output .hlist dd dt:last-child::after,.mw-parser-output .hlist dd li:last-child::after,.mw-parser-output .hlist dt dd:last-child::after,.mw-parser-output .hlist dt dt:last-child::after,.mw-parser-output .hlist dt li:last-child::after,.mw-parser-output .hlist li dd:last-child::after,.mw-parser-output .hlist li dt:last-child::after,.mw-parser-output .hlist li li:last-child::after{content:")";font-weight:normal}.mw-parser-output .hlist ol{counter-reset:listitem}.mw-parser-output .hlist ol>li{counter-increment:listitem}.mw-parser-output .hlist ol>li::before{content:" "counter(listitem)"\a0 "}.mw-parser-output .hlist dd ol>li:first-child::before,.mw-parser-output .hlist dt ol>li:first-child::before,.mw-parser-output .hlist li ol>li:first-child::before{content:" ("counter(listitem)"\a0 "}
   </style>
   <style data-mw-deduplicate="TemplateStyles:r1246091330">
    .mw-parser-output .sidebar{width:22em;float:right;clear:right;margin:0.5em 0 1em 1em;background:var(--background-color-neutral-subtle,#f8f9fa);border:1px solid var(--border-color-base,#a2a9b1);padding:0.2em;text-align:center;line-height:1.4em;font-size:88%;border-collapse:collapse;display:table}body.skin-minerva .mw-parser-output .sidebar{display:table!important;float:right!important;margin:0.5em 0 1em 1em!important}.mw-parser-output .sidebar-subgroup{width:100%;margin:0;border-spacing:0}.mw-parser-output .sidebar-left{float:left;clear:left;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-none{float:none;clear:both;margin:0.5em 1em 1em 0}.mw-parser-output .sidebar-outer-title{padding:0 0.4em 0.2em;font-size:125%;line-height:1.2em;font-weight:bold}.mw-parser-output .sidebar-top-image{padding:0.4em}.mw-parser-output .sidebar-top-caption,.mw-parser-output .sidebar-pretitle-with-top-image,.mw-parser-output .sidebar-caption{padding:0.2em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-pretitle{padding:0.4em 0.4em 0;line-height:1.2em}.mw-parser-output .sidebar-title,.mw-parser-output .sidebar-title-with-pretitle{padding:0.2em 0.8em;font-size:145%;line-height:1.2em}.mw-parser-output .sidebar-title-with-pretitle{padding:0.1em 0.4em}.mw-parser-output .sidebar-image{padding:0.2em 0.4em 0.4em}.mw-parser-output .sidebar-heading{padding:0.1em 0.4em}.mw-parser-output .sidebar-content{padding:0 0.5em 0.4em}.mw-parser-output .sidebar-content-with-subgroup{padding:0.1em 0.4em 0.2em}.mw-parser-output .sidebar-above,.mw-parser-output .sidebar-below{padding:0.3em 0.8em;font-weight:bold}.mw-parser-output .sidebar-collapse .sidebar-above,.mw-parser-output .sidebar-collapse .sidebar-below{border-top:1px solid #aaa;border-bottom:1px solid #aaa}.mw-parser-output .sidebar-navbar{text-align:right;font-size:115%;padding:0 0.4em 0.4em}.mw-parser-output .sidebar-list-title{padding:0 0.4em;text-align:left;font-weight:bold;line-height:1.6em;font-size:105%}.mw-parser-output .sidebar-list-title-c{padding:0 0.4em;text-align:center;margin:0 3.3em}@media(max-width:640px){body.mediawiki .mw-parser-output .sidebar{width:100%!important;clear:both;float:none!important;margin-left:0!important;margin-right:0!important}}body.skin--responsive .mw-parser-output .sidebar a>img{max-width:none!important}@media screen{html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-night .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-list-title,html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle{background:transparent!important}html.skin-theme-clientpref-os .mw-parser-output .sidebar:not(.notheme) .sidebar-title-with-pretitle a{color:var(--color-progressive)!important}}@media print{body.ns-0 .mw-parser-output .sidebar{display:none!important}}
   </style>
   <link href="mw-data:TemplateStyles:r1129693374" rel="mw-deduplicated-inline-style"/>
   <link href="mw-data:TemplateStyles:r1129693374" rel="mw-deduplicated-inline-style"/>
   <link href="mw-data:TemplateStyles:r1129693374" rel="mw-deduplicated-inline-style"/>
   <link href="mw-data:TemplateStyles:r1129693374" rel="mw-deduplicated-inline-style"/>
   <table class="sidebar nomobile nowraplinks">
    <tbody>
     <tr>
      <td class="sidebar-pretitle">
       Part of
       <a href="/wiki/Category:Automation" title="Category:Automation">
        a series
       </a>
       on
      </td>
     </tr>
     <tr>
      <th class="sidebar-title-with-pretitle">
       <a href="/wiki/Outline_of_automation" title="Outline of automation">
        Automation
       </a>
      </th>
     </tr>
     <tr>
      <td class="sidebar-image">
       <span typeof="mw:File/Frameless">
        <a class="mw-file-description" href="/wiki/File:Robot_arm_icon.svg">
         <img class="mw-file-element" data-file-height="233" data-file-width="246" decoding="async" height="95" src="//upload.wikimedia.org/wikipedia/commons/thumb/8/82/Robot_arm_icon.svg/100px-Robot_arm_icon.svg.png" srcset="//upload.wikimedia.org/wikipedia/commons/thumb/8/82/Robot_arm_icon.svg/150px-Robot_arm_icon.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/8/82/Robot_arm_icon.svg/200px-Robot_arm_icon.svg.png 2x" width="100"/>
        </a>
       </span>
      </td>
     </tr>
     <tr>
      <th class="sidebar-heading">
       <a href="/wiki/Automation" title="Automation">
        Automation in general
       </a>
      </th>
     </tr>
     <tr>
      <td class="sidebar-content">
       <div class="hlist">
        <ul>
         <li>
          <a href="/wiki/Automated_cash_handling" title="Automated cash handling">
           Banking
          </a>
         </li>
         <li>
          <a href="/wiki/Building_automation" title="Building automation">
           Building
          </a>
          <ul>
           <li>
            <a href="/wiki/Home_automation" title="Home automation">
             Home
            </a>
           </li>
          </ul>
         </li>
         <li>
          <a class="mw-redirect" href="/wiki/Automated_highway_system" title="Automated highway system">
           Highway system
          </a>
         </li>
         <li>
          <a href="/wiki/Laboratory_automation" title="Laboratory automation">
           Laboratory
          </a>
         </li>
         <li>
          <a href="/wiki/Integrated_library_system" title="Integrated library system">
           Library
          </a>
         </li>
         <li>
          <a href="/wiki/Broadcast_automation" title="Broadcast automation">
           Broadcast
          </a>
         </li>
         <li>
          <a href="/wiki/Mix_automation" title="Mix automation">
           Mix
          </a>
         </li>
         <li>
          <a href="/wiki/Automated_pool_cleaner" title="Automated pool cleaner">
           Pool cleaner
          </a>
         </li>
         <li>
          <a href="/wiki/Pop_music_automation" title="Pop music automation">
           Pop music
          </a>
         </li>
         <li>
          <a href="/wiki/Automated_reasoning" title="Automated reasoning">
           Reasoning
          </a>
         </li>
         <li>
          <a href="/wiki/Semi-automation" title="Semi-automation">
           Semi-automation
          </a>
         </li>
         <li>
          Telephone
          <ul>
           <li>
            <a href="/wiki/Automated_attendant" title="Automated attendant">
             Attendant
            </a>
           </li>
           <li>
            <a href="/wiki/Telephone_switchboard" title="Telephone switchboard">
             Switchboard
            </a>
           </li>
          </ul>
         </li>
         <li>
          <a href="/wiki/Automated_teller_machine" title="Automated teller machine">
           Teller machine
          </a>
         </li>
         <li>
          <a href="/wiki/Vehicular_automation" title="Vehicular automation">
           Vehicular
          </a>
         </li>
         <li>
          <a href="/wiki/Vending_machine" title="Vending machine">
           Vending machine
          </a>
         </li>
        </ul>
       </div>
      </td>
     </tr>
     <tr>
      <th class="sidebar-heading">
       <a href="/wiki/Robotics" title="Robotics">
        Robotics
       </a>
       and
       <a href="/wiki/Robot" title="Robot">
        robots
       </a>
      </th>
     </tr>
     <tr>
      <td class="sidebar-content">
       <div class="hlist">
        <ul>
         <li>
          <a href="/wiki/Domestic_robot" title="Domestic robot">
           Domestic
          </a>
          <ul>
           <li>
            <a href="/wiki/Robotic_vacuum_cleaner" title="Robotic vacuum cleaner">
             Vacuum cleaner
            </a>
            <ul>
             <li>
              <a href="/wiki/Roomba" title="Roomba">
               Roomba
              </a>
             </li>
            </ul>
           </li>
           <li>
            <a href="/wiki/Robotic_lawn_mower" title="Robotic lawn mower">
             Lawn mower
            </a>
           </li>
          </ul>
         </li>
         <li>
          <a href="/wiki/Automated_guided_vehicle" title="Automated guided vehicle">
           Guided vehicle
          </a>
         </li>
         <li>
          <a href="/wiki/Industrial_robot" title="Industrial robot">
           Industrial
          </a>
          <ul>
           <li>
            <a href="/wiki/Paint_robot" title="Paint robot">
             Paint
            </a>
           </li>
          </ul>
         </li>
         <li>
          <a href="/wiki/Operational_design_domain" title="Operational design domain">
           ODD
          </a>
         </li>
        </ul>
       </div>
      </td>
     </tr>
     <tr>
      <th class="sidebar-heading">
       Impact of automation
      </th>
     </tr>
     <tr>
      <td class="sidebar-content hlist">
       <div class="hlist">
        <ul>
         <li>
          <a href="/wiki/Manumation" title="Manumation">
           Manumation
          </a>
         </li>
         <li>
          <a href="/wiki/Out-of-the-loop_performance_problem" title="Out-of-the-loop performance problem">
           OOL
          </a>
          <ul>
           <li>
            <a class="mw-selflink selflink">
             Bias
            </a>
           </li>
          </ul>
         </li>
         <li>
          <a href="/wiki/Impact_of_self-driving_cars" title="Impact of self-driving cars">
           Self-driving cars
          </a>
         </li>
         <li>
          <a href="/wiki/Technological_unemployment" title="Technological unemployment">
           Technological unemployment
          </a>
          <ul>
           <li>
            <a href="/wiki/Jobless_recovery" title="Jobless recovery">
             Jobless recovery
            </a>
           </li>
           <li>
            <a href="/wiki/Post-work_society" title="Post-work society">
             Post-work society
            </a>
           </li>
          </ul>
         </li>
         <li>
          <a href="/wiki/Automated_threat" title="Automated threat">
           Threat
          </a>
         </li>
        </ul>
       </div>
      </td>
     </tr>
     <tr>
      <th class="sidebar-heading">
       Trade shows and awards
      </th>
     </tr>
     <tr>
      <td class="sidebar-content hlist">
       <div class="hlist">
        <ul>
         <li>
          <a href="/wiki/Asia_and_South_Pacific_Design_Automation_Conference" title="Asia and South Pacific Design Automation Conference">
           ASP-DAC
          </a>
         </li>
         <li>
          <a href="/wiki/Design_Automation_Conference" title="Design Automation Conference">
           DAC
          </a>
         </li>
         <li>
          <a href="/wiki/Design_Automation_and_Test_in_Europe" title="Design Automation and Test in Europe">
           DATE
          </a>
         </li>
         <li>
          <a href="/wiki/IEEE_Robotics_and_Automation_Award" title="IEEE Robotics and Automation Award">
           IEEE Robotics and Automation Award
          </a>
         </li>
         <li>
          <a href="/wiki/International_Conference_on_Computer-Aided_Design" title="International Conference on Computer-Aided Design">
           ICCAD
          </a>
         </li>
        </ul>
       </div>
      </td>
     </tr>
     <tr>
      <td class="sidebar-navbar">
       <link href="mw-data:TemplateStyles:r1129693374" rel="mw-deduplicated-inline-style"/>
       <style data-mw-deduplicate="TemplateStyles:r1239400231">
        .mw-parser-output .navbar{display:inline;font-size:88%;font-weight:normal}.mw-parser-output .navbar-collapse{float:left;text-align:left}.mw-parser-output .navbar-boxtext{word-spacing:0}.mw-parser-output .navbar ul{display:inline-block;white-space:nowrap;line-height:inherit}.mw-parser-output .navbar-brackets::before{margin-right:-0.125em;content:"[ "}.mw-parser-output .navbar-brackets::after{margin-left:-0.125em;content:" ]"}.mw-parser-output .navbar li{word-spacing:-0.125em}.mw-parser-output .navbar a>span,.mw-parser-output .navbar a>abbr{text-decoration:inherit}.mw-parser-output .navbar-mini abbr{font-variant:small-caps;border-bottom:none;text-decoration:none;cursor:inherit}.mw-parser-output .navbar-ct-full{font-size:114%;margin:0 7em}.mw-parser-output .navbar-ct-mini{font-size:114%;margin:0 4em}html.skin-theme-clientpref-night .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}@media(prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .navbar li a abbr{color:var(--color-base)!important}}@media print{.mw-parser-output .navbar{display:none!important}}
       </style>
       <div class="navbar plainlinks hlist navbar-mini">
        <ul>
         <li class="nv-view">
          <a href="/wiki/Template:Automation" title="Template:Automation">
           <abbr title="View this template">
            v
           </abbr>
          </a>
         </li>
         <li class="nv-talk">
          <a href="/wiki/Template_talk:Automation" title="Template talk:Automation">
           <abbr title="Discuss this template">
            t
           </abbr>
          </a>
         </li>
         <li class="nv-edit">
          <a href="/wiki/Special:EditPage/Template:Automation" title="Special:EditPage/Template:Automation">
           <abbr title="Edit this template">
            e
           </abbr>
          </a>
         </li>
        </ul>
       </div>
      </td>
     </tr>
    </tbody>
   </table>
   <p>
    <b>
     Automation bias
    </b>
    is the propensity for humans to favor suggestions from automated
    <a href="/wiki/Decision_support_system" title="Decision support system">
     decision-making systems
    </a>
    and to ignore contradictory information made without automation, even if it is correct.
    <sup class="reference" id="cite_ref-1">
     <a href="#cite_note-1">
      <span class="cite-bracket">
       [
      </span>
      1
      <span class="cite-bracket">
       ]
      </span>
     </a>
    </sup>
    Automation bias stems from the
    <a href="/wiki/Social_psychology" title="Social psychology">
     social psychology
    </a>
    literature that found a bias in human-human interaction that showed that people assign more positive evaluations to decisions made by humans than to a neutral object.
    <sup class="reference" id="cite_ref-2">
     <a href="#cite_note-2">
      <span class="cite-bracket">
       [
      </span>
      2
      <span class="cite-bracket">
       ]
      </span>
     </a>
    </sup>
    The same type of positivity bias has been found for human-automation interaction,
    <sup class="reference" id="cite_ref-3">
     <a href="#cite_note-3">
      <span class="cite-bracket">
       [
      </span>
      3
      <span class="cite-bracket">
       ]
      </span>
     </a>
    </sup>
    where the
    <a class="mw-redirect" href="/wiki/Automated_decision" title="Automated decision">
     automated decisions
    </a>
    are rated more positively than neutral.
    <sup class="reference" id="cite_ref-4">
     <a href="#cite_note-4">
      <span class="cite-bracket">
       [
      </span>
      4
      <span class="cite-bracket">
       ]
      </span>
     </a>
    </sup>
    This has become a growing problem for decision making as
    <a class="mw-redirect" href="/wiki/Intensive_care_units" title="Intensive care units">
     intensive care units
    </a>
    ,
    <a class="mw-redirect" href="/wiki/Nuclear_power_plants" title="Nuclear power plants">
     nuclear power plants
    </a>
    , and
    <a href="/wiki/Cockpit" title="Cockpit">
     aircraft cockpits
    </a>
    have increasingly integrated computerized system monitors and decision aids to mostly factor out possible human error. Errors of automation bias tend to occur when decision-making is dependent on computers or other automated aids and the human is in an observatory role but able to make decisions. Examples of automation bias range from urgent matters like flying a plane on automatic pilot to such mundane matters as the use of
    <a class="mw-redirect" href="/wiki/Spell-checking" title="Spell-checking">
     spell-checking
    </a>
    programs.
    <sup class="reference" id="cite_ref-University_of_Illinois_at_Chicago_5-0">
     <a href="#cite_note-University_of_Illinois_at_Chicago-5">
      <span class="cite-bracket">
       [
      </span>
      5
      <span class="cite-bracket">
       ]
      </span>
     </a>
    </sup>
   </p>
   <meta property="mw:PageProp/toc">
    <div class="mw-heading mw-heading2">
     <h2 id="Disuse_and_misuse">
      Disuse and misuse
     </h2>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=1" title="Edit section: Disuse and misuse">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     An operator's trust in the system can also lead to different interactions with the system, including system use, misuse, disuse, and abuse.
     <sup class="reference" id="cite_ref-ParasuramanRiley1997_6-0">
      <a href="#cite_note-ParasuramanRiley1997-6">
       <span class="cite-bracket">
        [
       </span>
       6
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     <sup class="noprint Inline-Template" style="margin-left:0.1em; white-space:nowrap;">
      [
      <i>
       <a href="/wiki/Wikipedia:Vagueness" title="Wikipedia:Vagueness">
        <span title="This information is too vague. (December 2019)">
         vague
        </span>
       </a>
      </i>
      ]
     </sup>
    </p>
    <p>
     The tendency toward overreliance on automated aids is known as "automation misuse".
     <sup class="reference" id="cite_ref-ParasuramanRiley1997_6-1">
      <a href="#cite_note-ParasuramanRiley1997-6">
       <span class="cite-bracket">
        [
       </span>
       6
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     <sup class="reference" id="cite_ref-international_Journal_of_Aviation_Psychology_7-0">
      <a href="#cite_note-international_Journal_of_Aviation_Psychology-7">
       <span class="cite-bracket">
        [
       </span>
       7
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     Misuse of automation can be seen when a user fails to properly monitor an automated system, or when the automated system is used when it should not be. This is in contrast to disuse, where the user does not properly utilize the automation either by turning it off or ignoring it. Both misuse and disuse can be problematic, but automation bias is directly related to misuse of the automation through either too much trust in the abilities of the system, or defaulting to using heuristics. Misuse can lead to lack of monitoring of the automated system or blind agreement with an automation suggestion, categorized by two types of errors, errors of omission and errors of commission, respectively.
     <sup class="reference" id="cite_ref-:0_8-0">
      <a href="#cite_note-:0-8">
       <span class="cite-bracket">
        [
       </span>
       8
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     <sup class="reference" id="cite_ref-9">
      <a href="#cite_note-9">
       <span class="cite-bracket">
        [
       </span>
       9
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     <sup class="reference" id="cite_ref-ParasuramanRiley1997_6-2">
      <a href="#cite_note-ParasuramanRiley1997-6">
       <span class="cite-bracket">
        [
       </span>
       6
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     Automation use and disuse can also influence stages of information processing: information acquisition, information analysis, decision making and
     <a href="/wiki/Action_selection" title="Action selection">
      action selection
     </a>
     , and action implementation.
     <sup class="reference" id="cite_ref-Wickens2015_10-0">
      <a href="#cite_note-Wickens2015-10">
       <span class="cite-bracket">
        [
       </span>
       10
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     For example, information acquisition, the first step in information processing, is the process by which a user registers input via the senses.
     <sup class="reference" id="cite_ref-Wickens2015_10-1">
      <a href="#cite_note-Wickens2015-10">
       <span class="cite-bracket">
        [
       </span>
       10
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     An automated engine gauge might assist the user with information acquisition through simple interface features—such as highlighting changes in the engine's performance—thereby directing the user's selective attention. When faced with issues originating from an aircraft, pilots may tend to overtrust an aircraft's engine gauges, losing sight of other possible malfunctions not related to the engine. This attitude is a form of automation complacency and misuse. If, however, the pilot devotes time to interpret the engine gauge, and manipulate the aircraft accordingly, only to discover that the flight turbulence has not changed, the pilot may be inclined to ignore future error recommendations conveyed by an engine gauge—a form of automation complacency leading to disuse.
    </p>
    <div class="mw-heading mw-heading2">
     <h2 id="Errors_of_commission_and_omission">
      Errors of commission and omission
     </h2>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=2" title="Edit section: Errors of commission and omission">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     Automation bias can take the form of commission errors, which occur when users follow an automated directive without taking into account other sources of information. Conversely, omission errors occur when automated devices fail to detect or indicate problems and the user does not notice because they are not properly monitoring the system.
     <sup class="reference" id="cite_ref-Sagepub_11-0">
      <a href="#cite_note-Sagepub-11">
       <span class="cite-bracket">
        [
       </span>
       11
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     Errors of omission have been shown to result from cognitive vigilance decrements, while errors of commission result from a combination of a failure to take information into account and an excessive trust in the reliability of automated aids.
     <sup class="reference" id="cite_ref-University_of_Illinois_at_Chicago_5-1">
      <a href="#cite_note-University_of_Illinois_at_Chicago-5">
       <span class="cite-bracket">
        [
       </span>
       5
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     Errors of commission occur for three reasons: (1) overt redirection of attention away from the automated aid; (2) diminished attention to the aid; (3) active discounting of information that counters the aid's recommendations.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-0">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     Omission errors occur when the human decision-maker fails to notice an automation failure, either due to low vigilance or overtrust in the system.
     <sup class="reference" id="cite_ref-University_of_Illinois_at_Chicago_5-2">
      <a href="#cite_note-University_of_Illinois_at_Chicago-5">
       <span class="cite-bracket">
        [
       </span>
       5
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     For example, a spell-checking program incorrectly marking a word as misspelled and suggesting an alternative would be an error of commission, and a spell-checking program failing to notice a misspelled word would be an error of omission. In these cases, automation bias could be observed by a user accepting the alternative word without consulting a dictionary, or a user not noticing the incorrectly misspelled word and assuming all the words are correct without reviewing the words.
    </p>
    <p>
     Training that focused on the reduction of automation bias and related problems has been shown to lower the rate of commission errors, but not of omission errors.
     <sup class="reference" id="cite_ref-University_of_Illinois_at_Chicago_5-3">
      <a href="#cite_note-University_of_Illinois_at_Chicago-5">
       <span class="cite-bracket">
        [
       </span>
       5
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading2">
     <h2 id="Factors">
      Factors
     </h2>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=3" title="Edit section: Factors">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     The presence of automatic aids, as one source puts it, "diminishes the likelihood that decision makers will either make the cognitive effort to seek other diagnostic information or process all available information in cognitively complex ways." It also renders users more likely to conclude their assessment of a situation too hastily after being prompted by an automatic aid to take a specific course of action.
     <sup class="reference" id="cite_ref-international_Journal_of_Aviation_Psychology_7-1">
      <a href="#cite_note-international_Journal_of_Aviation_Psychology-7">
       <span class="cite-bracket">
        [
       </span>
       7
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     According to one source, there are three main factors that lead to automation bias. First, the human tendency to choose the least cognitive approach to decision-making, which is called the
     <a href="/wiki/Cognitive_miser" title="Cognitive miser">
      cognitive miser
     </a>
     hypothesis. Second, the tendency of humans to view automated aids as having an analytical ability superior to their own. Third, the tendency of humans to reduce their own effort when sharing tasks, either with another person or with an automated aid.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-1">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     Other factors leading to an over-reliance on automation and thus to automation bias include inexperience in a task (though inexperienced users tend to be most benefited by automated decision support systems), lack of confidence in one's own abilities, a lack of readily available alternative information, or desire to save time and effort on complex tasks or high workloads.
     <sup class="reference" id="cite_ref-goddard2012_13-0">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     <sup class="reference" id="cite_ref-Alberdi2009_14-0">
      <a href="#cite_note-Alberdi2009-14">
       <span class="cite-bracket">
        [
       </span>
       14
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     <sup class="reference" id="cite_ref-goddard2014_15-0">
      <a href="#cite_note-goddard2014-15">
       <span class="cite-bracket">
        [
       </span>
       15
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     <sup class="reference" id="cite_ref-:0_8-1">
      <a href="#cite_note-:0-8">
       <span class="cite-bracket">
        [
       </span>
       8
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     It has been shown that people who have greater confidence in their own decision-making abilities tend to be less reliant on external automated support, while those with more trust in decision support systems (DSS) were more dependent upon it.
     <sup class="reference" id="cite_ref-goddard2012_13-1">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id="Screen_design">
      Screen design
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=4" title="Edit section: Screen design">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     One study, published in the
     <i>
      <a href="/wiki/Journal_of_the_American_Medical_Informatics_Association" title="Journal of the American Medical Informatics Association">
       Journal of the American Medical Informatics Association
      </a>
     </i>
     , found that the position and prominence of advice on a screen can impact the likelihood of automation bias, with prominently displayed advice, correct or not, is more likely to be followed; another study, however, seemed to discount the importance of this factor.
     <sup class="reference" id="cite_ref-goddard2012_13-2">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     According to another study, a greater amount of on-screen detail can make users less "conservative" and thus increase the likelihood of automation bias.
     <sup class="reference" id="cite_ref-goddard2012_13-3">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     One study showed that making individuals accountable for their performance or the accuracy of their decisions reduced automation bias.
     <sup class="reference" id="cite_ref-University_of_Illinois_at_Chicago_5-4">
      <a href="#cite_note-University_of_Illinois_at_Chicago-5">
       <span class="cite-bracket">
        [
       </span>
       5
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id="Availability">
      Availability
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=5" title="Edit section: Availability">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     "The availability of automated decision aids," states one study by
     <a href="/wiki/Linda_Skitka" title="Linda Skitka">
      Linda Skitka
     </a>
     , "can sometimes feed into the general human tendency to travel the road of least cognitive effort."
     <sup class="reference" id="cite_ref-University_of_Illinois_at_Chicago_5-5">
      <a href="#cite_note-University_of_Illinois_at_Chicago-5">
       <span class="cite-bracket">
        [
       </span>
       5
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id="Awareness_of_process">
      Awareness of process
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=6" title="Edit section: Awareness of process">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     One study also found that when users are made aware of the reasoning process employed by a decision support system, they are likely to adjust their reliance accordingly, thus reducing automation bias.
     <sup class="reference" id="cite_ref-goddard2012_13-4">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id="Team_vs._individual">
      Team vs. individual
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=7" title="Edit section: Team vs. individual">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     The performance of jobs by crews instead of individuals acting alone does not necessarily eliminate automation bias.
     <sup class="reference" id="cite_ref-The_International_Journal_of_Aviation_Psychology_16-0">
      <a href="#cite_note-The_International_Journal_of_Aviation_Psychology-16">
       <span class="cite-bracket">
        [
       </span>
       16
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     <sup class="reference" id="cite_ref-Sagepub_11-1">
      <a href="#cite_note-Sagepub-11">
       <span class="cite-bracket">
        [
       </span>
       11
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     One study has shown that when automated devices failed to detect system irregularities, teams were no more successful than solo performers at responding to those irregularities.
     <sup class="reference" id="cite_ref-University_of_Illinois_at_Chicago_5-6">
      <a href="#cite_note-University_of_Illinois_at_Chicago-5">
       <span class="cite-bracket">
        [
       </span>
       5
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id="Training">
      Training
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=8" title="Edit section: Training">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     Training that focuses on automation bias in
     <a href="/wiki/Aviation" title="Aviation">
      aviation
     </a>
     has succeeded in reducing omission errors by student pilots.
     <sup class="reference" id="cite_ref-The_International_Journal_of_Aviation_Psychology_16-1">
      <a href="#cite_note-The_International_Journal_of_Aviation_Psychology-16">
       <span class="cite-bracket">
        [
       </span>
       16
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     <sup class="reference" id="cite_ref-Sagepub_11-2">
      <a href="#cite_note-Sagepub-11">
       <span class="cite-bracket">
        [
       </span>
       11
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id='Automation_failure_and_"learned_carelessness"'>
      <span id="Automation_failure_and_.22learned_carelessness.22">
      </span>
      Automation failure and "learned carelessness"
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=9" title='Edit section: Automation failure and "learned carelessness"'>
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     It has been shown that automation failure is followed by a drop in operator trust, which in turn is succeeded by a slow recovery of trust. The decline in trust after an initial automation failure has been described as the first-failure effect.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-2">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     By the same token, if automated aids prove to be highly reliable over time, the result is likely to be a heightened level of automation bias. This is called "learned carelessness."
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-3">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id="Provision_of_system_confidence_information">
      Provision of system confidence information
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=10" title="Edit section: Provision of system confidence information">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     In cases where system confidence information is provided to users, that information itself can become a factor in automation bias.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-4">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id="External_pressures">
      External pressures
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=11" title="Edit section: External pressures">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     Studies have shown that the more external pressures are exerted on an individual's cognitive capacity, the more he or she may rely on external support.
     <sup class="reference" id="cite_ref-goddard2012_13-5">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id="Definitional_problems">
      Definitional problems
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=12" title="Edit section: Definitional problems">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     Although automation bias has been the subject of many studies, there continue to be complaints that automation bias remains ill-defined and that reporting of incidents involving automation bias is unsystematic.
     <sup class="reference" id="cite_ref-goddard2012_13-6">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     <sup class="reference" id="cite_ref-:0_8-2">
      <a href="#cite_note-:0-8">
       <span class="cite-bracket">
        [
       </span>
       8
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     A review of various automation bias studies categorized the different types of tasks where automated aids were used as well as what function the automated aids served. Tasks where automated aids were used were categorized as monitoring tasks, diagnosis tasks, or treatment tasks. Types of automated assistance were listed as Alerting automation, which track important changes and alert the user, Decision support automation, which may provide a diagnosis or recommendation, or Implementation automation, where the automated aid performs a specified task.
     <sup class="reference" id="cite_ref-:0_8-3">
      <a href="#cite_note-:0-8">
       <span class="cite-bracket">
        [
       </span>
       8
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading2">
     <h2 id="Automation-induced_complacency">
      Automation-induced complacency
     </h2>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=13" title="Edit section: Automation-induced complacency">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     The concept of automation bias is viewed as overlapping with automation-induced complacency, also known more simply as automation complacency. Like automation bias, it is a consequence of the misuse of automation and involves problems of attention. While automation bias involves a tendency to trust decision-support systems, automation complacency involves insufficient attention to and monitoring of automation output, usually because that output is viewed as reliable.
     <sup class="reference" id="cite_ref-goddard2012_13-7">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     "Although the  concepts  of complacency  and automation bias have been discussed separately as if they were independent," writes one expert, "they share several commonalities, suggesting they reflect different aspects of the same  kind of automation misuse." It has been proposed, indeed, that the concepts of complacency and automation bias be combined into a single "integrative concept" because these two concepts "might represent different manifestations of overlapping automation-induced phenomena" and because "automation-induced complacency and automation bias represent closely linked theoretical concepts that show considerable overlap with respect to the underlying processes."
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-5">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     Automation complacency has been defined as "poorer detection of system malfunctions under automation compared with under manual control."
     <a href="/wiki/NASA" title="NASA">
      NASA's
     </a>
     <a href="/wiki/Aviation_Safety_Reporting_System" title="Aviation Safety Reporting System">
      Aviation Safety Reporting System
     </a>
     (ASRS) defines complacency as "self-satisfaction that  may result in non-vigilance based on an unjustified assumption of satisfactory system state." Several studies have indicated that it occurs most often when operators are engaged in both manual and automated tasks at the same time. In turn, the operators' perceptions of the automated system's reliability can influence the way in which the operator interacts with the system. Endsley (2017) describes how high system reliability can lead users to disengage from monitoring systems, thereby increasing monitoring errors, decreasing situational awareness, and interfering with an operator's ability to re-assume control of the system in the event performance limitations have been exceeded.
     <sup class="reference" id="cite_ref-17">
      <a href="#cite_note-17">
       <span class="cite-bracket">
        [
       </span>
       17
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     This complacency can be sharply reduced when automation reliability varies over time instead of remaining constant, but is not reduced by experience and practice. Both expert and inexpert participants can exhibit automation bias as well as automation complacency. Neither of these problems can be easily overcome by training.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-6">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     The term "automation complacency" was first used in connection with aviation accidents or incidents in which
     <a href="/wiki/Aircraft_pilot" title="Aircraft pilot">
      pilots
     </a>
     ,
     <a href="/wiki/Air_traffic_controller" title="Air traffic controller">
      air-traffic controllers
     </a>
     , or  other workers failed to check systems sufficiently, assuming that everything was fine when, in reality, an accident was about to occur. Operator complacency, whether or not automation-related, has long been recognized as a leading factor in air accidents.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-7">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     As such, perceptions of reliability, in general, can result in a form of
     <a href="/wiki/Ironies_of_Automation" title="Ironies of Automation">
      automation irony
     </a>
     , in which more automation can decrease cognitive workload but increase the opportunity for monitoring errors. In contrast, low automation can increase workload but decrease the opportunity for monitoring errors.
     <sup class="reference" id="cite_ref-18">
      <a href="#cite_note-18">
       <span class="cite-bracket">
        [
       </span>
       18
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     Take, for example, a pilot flying through inclement weather, in which continuous thunder interferes with the pilot's ability to understand information transmitted by an air traffic controller (ATC). Despite how much effort is allocated to understanding information transmitted by ATC, the pilot's performance is limited by the source of information needed for the task. The pilot therefore has to rely on automated gauges in the cockpit to understand flight path information. If the pilot perceives the automated gauges to be highly reliable, the amount of effort needed to understand ATC and automated gauges may decrease. Moreover, if the automated gauges are perceived to be highly reliable, the pilot may ignore those gauges to devote mental resources for deciphering information transmitted by ATC. In so doing, the pilot becomes a complacent monitor, thereby running the risk of missing critical information conveyed by the automated gauges. If, however, the pilot perceives the automated gauges to be unreliable, the pilot will now have to interpret information from ATC and automated gauges simultaneously. This creates scenarios in which the operator may be expending unnecessary cognitive resources when the automation is in fact reliable, but also increasing the odds of identifying potential errors in the weather gauges should they occur. To calibrate the pilot's perception of reliability, automation should be designed to maintain workload at appropriate levels while also ensuring the operator remains engaged with monitoring tasks. The operator should be less likely to disengage from monitoring when the system's reliability can change as compared to a system that has consistent reliability (Parasuraman, 1993).
     <sup class="reference" id="cite_ref-ParasuramanMolloy1993_19-0">
      <a href="#cite_note-ParasuramanMolloy1993-19">
       <span class="cite-bracket">
        [
       </span>
       19
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     To some degree, user complacency offsets the benefits of automation, and when an automated system's reliability level falls below a certain level, then automation will no longer be a net asset. One 2007 study suggested that this automation occurs when the reliability level reaches approximately 70%. Other studies have found that automation with a reliability level below 70% can be of use to persons with access to the raw information sources, which can be combined with the automation output to improve performance.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-8">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     <a href="/wiki/Death_by_GPS" title="Death by GPS">
      Death by GPS
     </a>
     , wherein the deaths of individuals is in part caused by following inaccurate GPS directions, is another example of automation complacency.
    </p>
    <div class="mw-heading mw-heading2">
     <h2 id="Sectors">
      Sectors
     </h2>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=14" title="Edit section: Sectors">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     Automation bias has been examined across many research fields.
     <sup class="reference" id="cite_ref-goddard2012_13-8">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     It can be a particularly major concern in aviation,
     <a href="/wiki/Medicine" title="Medicine">
      medicine
     </a>
     ,
     <a class="mw-redirect" href="/wiki/Process_control" title="Process control">
      process control
     </a>
     , and
     <a href="/wiki/Military" title="Military">
      military
     </a>
     <a class="mw-redirect" href="/wiki/Command-and-control" title="Command-and-control">
      command-and-control
     </a>
     operations.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-9">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id="Aviation">
      Aviation
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=15" title="Edit section: Aviation">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     At first, discussion of automation bias focused largely on aviation. Automated aids have played an increasing role in cockpits, taking a growing role in the control of such flight tasks as determining the most fuel-efficient routes, navigating, and detecting and diagnosing system malfunctions. The use of these aids, however, can lead to less attentive and less vigilant information seeking and processing on the part of human beings. In some cases, human beings may place more confidence in the misinformation provided by flight computers than in their own skills.
     <sup class="reference" id="cite_ref-international_Journal_of_Aviation_Psychology_7-2">
      <a href="#cite_note-international_Journal_of_Aviation_Psychology-7">
       <span class="cite-bracket">
        [
       </span>
       7
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     An important factor in aviation-related automation bias is the degree to which pilots perceive themselves as responsible for the tasks being carried out by automated aids. One study of pilots showed that the presence of a second crewmember in the cockpit did not affect automation bias. A 1994 study compared the impact of low and high levels of automation (LOA) on pilot performance, and concluded that pilots working with a high level spent less time reflecting independently on flight decisions.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-10">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     In another study, all of the pilots given false automated alerts that instructed them to shut off an engine did so, even though those same pilots insisted in an interview that they would not respond to such an alert by shutting down an engine, and would instead have reduced the power to idle. One 1998 study found that pilots with approximately 440 hours of flight experience detected more automation failures than did nonpilots, although both groups showed complacency effects. A 2001 study of pilots using a cockpit automation system, the
     <a href="/wiki/Engine-indicating_and_crew-alerting_system" title="Engine-indicating and crew-alerting system">
      Engine-indicating and crew-alerting system
     </a>
     (EICAS), showed evidence of complacency. The pilots detected fewer engine malfunctions when using the system than when performing the task manually.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-11">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     In a 2005 study, experienced air-traffic controllers used high-fidelity simulation of an
     <a href="/wiki/Air_traffic_control" title="Air traffic control">
      ATC
     </a>
     (Free Flight) scenario that involved the detection of conflicts among "self-separating" aircraft. They had access to an automated device that identified potential conflicts several minutes ahead of time. When the device failed near the end of the simulation process, considerably fewer controllers detected the conflict than when the situation was handled manually. Other studies have produced similar findings.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-12">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     Two studies of automation bias in aviation discovered a higher rate of commission errors than omission errors, while another aviation study found 55% omission rates and 0% commission rates.
     <sup class="reference" id="cite_ref-goddard2012_13-9">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     Automation-related omissions errors are especially common during the cruise phase. When a
     <a href="/wiki/China_Airlines" title="China Airlines">
      China Airlines
     </a>
     flight lost power in one engine, the autopilot attempted to correct for this problem by lowering the left wing, an action that hid the problem from the crew. When the autopilot was disengaged, the airplane rolled to the right and descended steeply, causing extensive damage. The 1983 shooting down of a Korean Airlines 747 over
     <a class="mw-redirect" href="/wiki/Soviet" title="Soviet">
      Soviet
     </a>
     airspace occurred because the Korean crew "relied on automation that had been inappropriately set up, and they never checked their progress manually."
     <sup class="reference" id="cite_ref-international_Journal_of_Aviation_Psychology_7-3">
      <a href="#cite_note-international_Journal_of_Aviation_Psychology-7">
       <span class="cite-bracket">
        [
       </span>
       7
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id="Health_care">
      Health care
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=16" title="Edit section: Health care">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     <a href="/wiki/Clinical_decision_support_system" title="Clinical decision support system">
      Clinical decision support systems
     </a>
     (CDSS) are designed to aid clinical decision-making. They have the potential to effect a great improvement in this regard, and to result in improved patient outcomes. Yet while CDSS, when used properly, bring about an overall improvement in performance, they also cause errors that may not be recognized owing to automation bias. One danger is that the incorrect advice given by these systems may cause users to change a correct decision that they have made on their own. Given the highly serious nature of some of the potential consequences of automation  bias in the health-care field, it is especially important to be aware of this problem when it occurs in clinical settings.
     <sup class="reference" id="cite_ref-goddard2012_13-10">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     Sometimes automation bias in clinical settings is a major problem that renders CDSS, on balance, counterproductive; sometimes it is  minor problem, with the benefits outweighing the damage done. One study found more automation bias among older users, but it was noted that could be a result not of age but of experience. Studies suggest, indeed, that familiarity with CDSS often leads to desensitization and habituation effects. Although automation bias occurs more often among persons who are inexperienced in a given task, inexperienced users exhibit the most performance improvement when they use CDSS. In one study, the use of CDSS improved clinicians' answers by 21%, from 29% to 50%, with 7% of correct non-CDSS answers being changed incorrectly.
     <sup class="reference" id="cite_ref-goddard2012_13-11">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     A 2005 study found that when
     <a href="/wiki/Primary_care_physician" title="Primary care physician">
      primary-care physicians
     </a>
     used electronic sources such as
     <a href="/wiki/PubMed" title="PubMed">
      PubMed
     </a>
     ,
     <a class="mw-redirect" href="/wiki/Medline" title="Medline">
      Medline
     </a>
     , and
     <a href="/wiki/Google" title="Google">
      Google
     </a>
     , there was a "small to medium" increase in correct answers, while in an equally small percentage of instances the physicians were misled by their use of those sources, and changed correct to incorrect answers.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-13">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <p>
     Studies in 2004 and 2008 that involved the effect of automated aids on diagnosis of
     <a href="/wiki/Breast_cancer" title="Breast cancer">
      breast cancer
     </a>
     found clear evidence of automation bias involving omission errors. Cancers diagnosed in 46% of cases without automated aids were discovered in only 21% of cases with automated aids that failed to identify the cancer.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-14">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id="Military">
      Military
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=17" title="Edit section: Military">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     Automation bias can be a crucial factor in the use of intelligent decision support systems for military command-and-control operations. One 2004 study found that automation bias effects have contributed to a number of fatal military decisions, including
     <a class="mw-redirect" href="/wiki/Friendly-fire" title="Friendly-fire">
      friendly-fire
     </a>
     killings during the
     <a href="/wiki/Iraq_War" title="Iraq War">
      Iraq War
     </a>
     . Researchers have sought to determine the proper level of automation for decision support systems in this field.
     <sup class="reference" id="cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-15">
      <a href="#cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="cite-bracket">
        [
       </span>
       12
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading3">
     <h3 id="Automotive">
      Automotive
     </h3>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=18" title="Edit section: Automotive">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     Automation complacency is also a challenge for automated driving systems in which the human only has to monitor the system or act as a fallback driver. This is for example discussed in the report of
     <a href="/wiki/National_Transportation_Safety_Board" title="National Transportation Safety Board">
      National Transportation Safety Board
     </a>
     about the fatal accident between an UBER test vehicle and pedestrian
     <a href="/wiki/Death_of_Elaine_Herzberg" title="Death of Elaine Herzberg">
      Elaine Herzberg
     </a>
     .
     <sup class="reference" id="cite_ref-20">
      <a href="#cite_note-20">
       <span class="cite-bracket">
        [
       </span>
       20
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading2">
     <h2 id="Correction">
      Correction
     </h2>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=19" title="Edit section: Correction">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <p>
     Automation bias can be mitigated by redesigning automated systems to reduce display prominence, decrease information complexity or couch assistance as supportive rather than directive information.
     <sup class="reference" id="cite_ref-goddard2012_13-12">
      <a href="#cite_note-goddard2012-13">
       <span class="cite-bracket">
        [
       </span>
       13
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     Training users on automated systems which introduce deliberate errors more effectively reduces automation bias than just telling them errors can occur.
     <sup class="reference" id="cite_ref-bahner2008_21-0">
      <a href="#cite_note-bahner2008-21">
       <span class="cite-bracket">
        [
       </span>
       21
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
     Excessively checking and questioning automated assistance can increase time pressure and task complexity, thus reducing benefit, so some automated decision support systems balance positive and negative effects rather than attempt to eliminate negative effects.
     <sup class="reference" id="cite_ref-Alberdi2009_14-1">
      <a href="#cite_note-Alberdi2009-14">
       <span class="cite-bracket">
        [
       </span>
       14
       <span class="cite-bracket">
        ]
       </span>
      </a>
     </sup>
    </p>
    <div class="mw-heading mw-heading2">
     <h2 id="See_also">
      See also
     </h2>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=20" title="Edit section: See also">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <ul>
     <li>
      <a href="/wiki/Algorithmic_bias" title="Algorithmic bias">
       Algorithmic bias
      </a>
     </li>
     <li>
      <a href="/wiki/Automation" title="Automation">
       Automation
      </a>
     </li>
     <li>
      <a href="/wiki/Intelligent_automation" title="Intelligent automation">
       Intelligent automation
      </a>
     </li>
     <li>
      <a href="/wiki/List_of_cognitive_biases" title="List of cognitive biases">
       List of cognitive biases
      </a>
     </li>
     <li>
      <a href="/wiki/Out-of-the-loop_performance_problem" title="Out-of-the-loop performance problem">
       Out-of-the-loop performance problem
      </a>
     </li>
    </ul>
    <div class="mw-heading mw-heading2">
     <h2 id="References">
      References
     </h2>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=21" title="Edit section: References">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <style data-mw-deduplicate="TemplateStyles:r1239543626">
     .mw-parser-output .reflist{margin-bottom:0.5em;list-style-type:decimal}@media screen{.mw-parser-output .reflist{font-size:90%}}.mw-parser-output .reflist .references{font-size:100%;margin-bottom:0;list-style-type:inherit}.mw-parser-output .reflist-columns-2{column-width:30em}.mw-parser-output .reflist-columns-3{column-width:25em}.mw-parser-output .reflist-columns{margin-top:0.3em}.mw-parser-output .reflist-columns ol{margin-top:0}.mw-parser-output .reflist-columns li{page-break-inside:avoid;break-inside:avoid-column}.mw-parser-output .reflist-upper-alpha{list-style-type:upper-alpha}.mw-parser-output .reflist-upper-roman{list-style-type:upper-roman}.mw-parser-output .reflist-lower-alpha{list-style-type:lower-alpha}.mw-parser-output .reflist-lower-greek{list-style-type:lower-greek}.mw-parser-output .reflist-lower-roman{list-style-type:lower-roman}
    </style>
    <div class="reflist reflist-columns references-column-width reflist-columns-2">
     <ol class="references">
      <li id="cite_note-1">
       <span class="mw-cite-backlink">
        <b>
         <a href="#cite_ref-1">
          ^
         </a>
        </b>
       </span>
       <span class="reference-text">
        <style data-mw-deduplicate="TemplateStyles:r1238218222">
         .mw-parser-output cite.citation{font-style:inherit;word-wrap:break-word}.mw-parser-output .citation q{quotes:"\"""\"""'""'"}.mw-parser-output .citation:target{background-color:rgba(0,127,255,0.133)}.mw-parser-output .id-lock-free.id-lock-free a{background:url("//upload.wikimedia.org/wikipedia/commons/6/65/Lock-green.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-limited.id-lock-limited a,.mw-parser-output .id-lock-registration.id-lock-registration a{background:url("//upload.wikimedia.org/wikipedia/commons/d/d6/Lock-gray-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .id-lock-subscription.id-lock-subscription a{background:url("//upload.wikimedia.org/wikipedia/commons/a/aa/Lock-red-alt-2.svg")right 0.1em center/9px no-repeat}.mw-parser-output .cs1-ws-icon a{background:url("//upload.wikimedia.org/wikipedia/commons/4/4c/Wikisource-logo.svg")right 0.1em center/12px no-repeat}body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-free a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-limited a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-registration a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .id-lock-subscription a,body:not(.skin-timeless):not(.skin-minerva) .mw-parser-output .cs1-ws-icon a{background-size:contain;padding:0 1em 0 0}.mw-parser-output .cs1-code{color:inherit;background:inherit;border:none;padding:inherit}.mw-parser-output .cs1-hidden-error{display:none;color:var(--color-error,#d33)}.mw-parser-output .cs1-visible-error{color:var(--color-error,#d33)}.mw-parser-output .cs1-maint{display:none;color:#085;margin-left:0.3em}.mw-parser-output .cs1-kern-left{padding-left:0.2em}.mw-parser-output .cs1-kern-right{padding-right:0.2em}.mw-parser-output .citation .mw-selflink{font-weight:inherit}@media screen{.mw-parser-output .cs1-format{font-size:95%}html.skin-theme-clientpref-night .mw-parser-output .cs1-maint{color:#18911f}}@media screen and (prefers-color-scheme:dark){html.skin-theme-clientpref-os .mw-parser-output .cs1-maint{color:#18911f}}
        </style>
        <cite class="citation book cs1" id="CITEREFCummings2004">
         Cummings, Mary (2004).
         <a class="external text" href="https://web.archive.org/web/20141101113133/http://web.mit.edu/aeroastro/labs/halab/papers/CummingsAIAAbias.pdf" rel="nofollow">
          "Automation Bias in Intelligent Time Critical Decision Support Systems"
         </a>
         <span class="cs1-format">
          (PDF)
         </span>
         .
         <a class="external text" href="http://web.mit.edu/aeroastro/labs/halab/papers/CummingsAIAAbias.pdf" rel="nofollow">
          <i>
           AIAA 1st Intelligent Systems Technical Conference
          </i>
         </a>
         <span class="cs1-format">
          (PDF)
         </span>
         .
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.2514%2F6.2004-6313" rel="nofollow">
          10.2514/6.2004-6313
         </a>
         .
         <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">
          ISBN
         </a>
         <a href="/wiki/Special:BookSources/978-1-62410-080-2" title="Special:BookSources/978-1-62410-080-2">
          <bdi>
           978-1-62410-080-2
          </bdi>
         </a>
         .
         <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">
          S2CID
         </a>
         <a class="external text" href="https://api.semanticscholar.org/CorpusID:10328335" rel="nofollow">
          10328335
         </a>
         . Archived from the original on 2014-11-01.
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Automation+Bias+in+Intelligent+Time+Critical+Decision+Support+Systems&amp;rft.btitle=AIAA+1st+Intelligent+Systems+Technical+Conference&amp;rft.date=2004&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A10328335%23id-name%3DS2CID&amp;rft_id=info%3Adoi%2F10.2514%2F6.2004-6313&amp;rft.isbn=978-1-62410-080-2&amp;rft.aulast=Cummings&amp;rft.aufirst=Mary&amp;rft_id=http%3A%2F%2Fwayback.archive.org%2Fweb%2F20141101113133%2Fhttp%3A%2F%2Fweb.mit.edu%2Faeroastro%2Flabs%2Fhalab%2Fpapers%2FCummingsAIAAbias.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
        <span class="cs1-maint citation-comment">
         <code class="cs1-code">
          {{
          <a href="/wiki/Template:Cite_book" title="Template:Cite book">
           cite book
          </a>
          }}
         </code>
         :  CS1 maint: bot: original URL status unknown (
         <a href="/wiki/Category:CS1_maint:_bot:_original_URL_status_unknown" title="Category:CS1 maint: bot: original URL status unknown">
          link
         </a>
         )
        </span>
       </span>
      </li>
      <li id="cite_note-2">
       <span class="mw-cite-backlink">
        <b>
         <a href="#cite_ref-2">
          ^
         </a>
        </b>
       </span>
       <span class="reference-text">
        Bruner, J. S., &amp; Tagiuri, R. 1954. "The perception of people". In G. Lindzey (Ed.),
        <i>
         <a class="external text" href="https://books.google.com/books?id=zO63AAAAIAAJ&amp;q=+The+perception+of+people" rel="nofollow">
          Handbook of social psychology (vol 2)
         </a>
         :
        </i>
        634-654. Reading, MA: Addison-Wesley.
       </span>
      </li>
      <li id="cite_note-3">
       <span class="mw-cite-backlink">
        <b>
         <a href="#cite_ref-3">
          ^
         </a>
        </b>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFMadhavanWiegmann2007">
         Madhavan, P.; Wiegmann, D. A. (2007-07-01). "Similarities and differences between human–human and human–automation trust: an integrative review".
         <i>
          Theoretical Issues in Ergonomics Science
         </i>
         .
         <b>
          8
         </b>
         (4): 277–301.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1080%2F14639220500337708" rel="nofollow">
          10.1080/14639220500337708
         </a>
         .
         <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">
          S2CID
         </a>
         <a class="external text" href="https://api.semanticscholar.org/CorpusID:39064140" rel="nofollow">
          39064140
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Theoretical+Issues+in+Ergonomics+Science&amp;rft.atitle=Similarities+and+differences+between+human%E2%80%93human+and+human%E2%80%93automation+trust%3A+an+integrative+review&amp;rft.volume=8&amp;rft.issue=4&amp;rft.pages=277-301&amp;rft.date=2007-07-01&amp;rft_id=info%3Adoi%2F10.1080%2F14639220500337708&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A39064140%23id-name%3DS2CID&amp;rft.aulast=Madhavan&amp;rft.aufirst=P.&amp;rft.au=Wiegmann%2C+D.+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-4">
       <span class="mw-cite-backlink">
        <b>
         <a href="#cite_ref-4">
          ^
         </a>
        </b>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFDzindoletPetersonPomrankyPierce2003">
         Dzindolet, Mary T.; Peterson, Scott A.; Pomranky, Regina A.; Pierce, Linda G.; Beck, Hall P. (2003). "The role of trust in automation reliance".
         <i>
          International Journal of Human-Computer Studies
         </i>
         .
         <b>
          58
         </b>
         (6): 697–718.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1016%2FS1071-5819%2803%2900038-7" rel="nofollow">
          10.1016/S1071-5819(03)00038-7
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Human-Computer+Studies&amp;rft.atitle=The+role+of+trust+in+automation+reliance&amp;rft.volume=58&amp;rft.issue=6&amp;rft.pages=697-718&amp;rft.date=2003&amp;rft_id=info%3Adoi%2F10.1016%2FS1071-5819%2803%2900038-7&amp;rft.aulast=Dzindolet&amp;rft.aufirst=Mary+T.&amp;rft.au=Peterson%2C+Scott+A.&amp;rft.au=Pomranky%2C+Regina+A.&amp;rft.au=Pierce%2C+Linda+G.&amp;rft.au=Beck%2C+Hall+P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-University_of_Illinois_at_Chicago-5">
       <span class="mw-cite-backlink">
        ^
        <a href="#cite_ref-University_of_Illinois_at_Chicago_5-0">
         <sup>
          <i>
           <b>
            a
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-University_of_Illinois_at_Chicago_5-1">
         <sup>
          <i>
           <b>
            b
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-University_of_Illinois_at_Chicago_5-2">
         <sup>
          <i>
           <b>
            c
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-University_of_Illinois_at_Chicago_5-3">
         <sup>
          <i>
           <b>
            d
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-University_of_Illinois_at_Chicago_5-4">
         <sup>
          <i>
           <b>
            e
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-University_of_Illinois_at_Chicago_5-5">
         <sup>
          <i>
           <b>
            f
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-University_of_Illinois_at_Chicago_5-6">
         <sup>
          <i>
           <b>
            g
           </b>
          </i>
         </sup>
        </a>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation web cs1" id="CITEREFSkitka">
         Skitka, Linda.
         <a class="external text" href="https://lskitka.people.uic.edu/styled-7/styled-14/index.html" rel="nofollow">
          "Automation"
         </a>
         .
         <i>
          University of Illinois
         </i>
         . University of Illinois at Chicago
         <span class="reference-accessdate">
          . Retrieved
          <span class="nowrap">
           16 January
          </span>
          2017
         </span>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=University+of+Illinois&amp;rft.atitle=Automation&amp;rft.aulast=Skitka&amp;rft.aufirst=Linda&amp;rft_id=https%3A%2F%2Flskitka.people.uic.edu%2Fstyled-7%2Fstyled-14%2Findex.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-ParasuramanRiley1997-6">
       <span class="mw-cite-backlink">
        ^
        <a href="#cite_ref-ParasuramanRiley1997_6-0">
         <sup>
          <i>
           <b>
            a
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-ParasuramanRiley1997_6-1">
         <sup>
          <i>
           <b>
            b
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-ParasuramanRiley1997_6-2">
         <sup>
          <i>
           <b>
            c
           </b>
          </i>
         </sup>
        </a>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFParasuramanRiley1997">
         Parasuraman, Raja; Riley, Victor (1997). "Humans and Automation: Use, Misuse, Disuse, Abuse".
         <i>
          Human Factors: The Journal of the Human Factors and Ergonomics Society
         </i>
         .
         <b>
          39
         </b>
         (2): 230–253.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1518%2F001872097778543886" rel="nofollow">
          10.1518/001872097778543886
         </a>
         .
         <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">
          S2CID
         </a>
         <a class="external text" href="https://api.semanticscholar.org/CorpusID:41149078" rel="nofollow">
          41149078
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Human+Factors%3A+The+Journal+of+the+Human+Factors+and+Ergonomics+Society&amp;rft.atitle=Humans+and+Automation%3A+Use%2C+Misuse%2C+Disuse%2C+Abuse&amp;rft.volume=39&amp;rft.issue=2&amp;rft.pages=230-253&amp;rft.date=1997&amp;rft_id=info%3Adoi%2F10.1518%2F001872097778543886&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A41149078%23id-name%3DS2CID&amp;rft.aulast=Parasuraman&amp;rft.aufirst=Raja&amp;rft.au=Riley%2C+Victor&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-international_Journal_of_Aviation_Psychology-7">
       <span class="mw-cite-backlink">
        ^
        <a href="#cite_ref-international_Journal_of_Aviation_Psychology_7-0">
         <sup>
          <i>
           <b>
            a
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-international_Journal_of_Aviation_Psychology_7-1">
         <sup>
          <i>
           <b>
            b
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-international_Journal_of_Aviation_Psychology_7-2">
         <sup>
          <i>
           <b>
            c
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-international_Journal_of_Aviation_Psychology_7-3">
         <sup>
          <i>
           <b>
            d
           </b>
          </i>
         </sup>
        </a>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFMosierSkitkaHeersBurdick1997">
         Mosier, Kathleen; Skitka, Linda; Heers, Susan; Burdick, Mark (1997).
         <a class="external text" href="https://www.researchgate.net/publication/11805395" rel="nofollow">
          "Automation Bias: Decision Making and Performance in High-Tech Cockpits"
         </a>
         .
         <i>
          International Journal of Aviation Psychology
         </i>
         .
         <b>
          8
         </b>
         (1): 47–63.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1207%2Fs15327108ijap0801_3" rel="nofollow">
          10.1207/s15327108ijap0801_3
         </a>
         .
         <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">
          PMID
         </a>
         <a class="external text" href="https://pubmed.ncbi.nlm.nih.gov/11540946" rel="nofollow">
          11540946
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Aviation+Psychology&amp;rft.atitle=Automation+Bias%3A+Decision+Making+and+Performance+in+High-Tech+Cockpits&amp;rft.volume=8&amp;rft.issue=1&amp;rft.pages=47-63&amp;rft.date=1997&amp;rft_id=info%3Adoi%2F10.1207%2Fs15327108ijap0801_3&amp;rft_id=info%3Apmid%2F11540946&amp;rft.aulast=Mosier&amp;rft.aufirst=Kathleen&amp;rft.au=Skitka%2C+Linda&amp;rft.au=Heers%2C+Susan&amp;rft.au=Burdick%2C+Mark&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F11805395&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-:0-8">
       <span class="mw-cite-backlink">
        ^
        <a href="#cite_ref-:0_8-0">
         <sup>
          <i>
           <b>
            a
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-:0_8-1">
         <sup>
          <i>
           <b>
            b
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-:0_8-2">
         <sup>
          <i>
           <b>
            c
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-:0_8-3">
         <sup>
          <i>
           <b>
            d
           </b>
          </i>
         </sup>
        </a>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFLyellCoiera2016">
         Lyell, David; Coiera, Enrico (August 2016).
         <a class="external text" href="https://academic.oup.com/jamia/article/24/2/423/2631492/Automation-bias-and-verification-complexity-a" rel="nofollow">
          "Automation bias and verification complexity: a systematic review"
         </a>
         .
         <i>
          Journal of the American Medical Informatics Association
         </i>
         .
         <b>
          24
         </b>
         (2): 424–431.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <span class="id-lock-free" title="Freely accessible">
          <a class="external text" href="https://doi.org/10.1093%2Fjamia%2Focw105" rel="nofollow">
           10.1093/jamia/ocw105
          </a>
         </span>
         .
         <a class="mw-redirect" href="/wiki/PMC_(identifier)" title="PMC (identifier)">
          PMC
         </a>
         <span class="id-lock-free" title="Freely accessible">
          <a class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC7651899" rel="nofollow">
           7651899
          </a>
         </span>
         .
         <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">
          PMID
         </a>
         <a class="external text" href="https://pubmed.ncbi.nlm.nih.gov/27516495" rel="nofollow">
          27516495
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+American+Medical+Informatics+Association&amp;rft.atitle=Automation+bias+and+verification+complexity%3A+a+systematic+review&amp;rft.volume=24&amp;rft.issue=2&amp;rft.pages=424-431&amp;rft.date=2016-08&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC7651899%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F27516495&amp;rft_id=info%3Adoi%2F10.1093%2Fjamia%2Focw105&amp;rft.aulast=Lyell&amp;rft.aufirst=David&amp;rft.au=Coiera%2C+Enrico&amp;rft_id=https%3A%2F%2Facademic.oup.com%2Fjamia%2Farticle%2F24%2F2%2F423%2F2631492%2FAutomation-bias-and-verification-complexity-a&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-9">
       <span class="mw-cite-backlink">
        <b>
         <a href="#cite_ref-9">
          ^
         </a>
        </b>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFTverskyKahneman1974">
         Tversky, A.; Kahneman, D. (1974). "Judgment under Uncertainty: Heuristics and Biases".
         <i>
          Science
         </i>
         .
         <b>
          185
         </b>
         (4157): 1124–1131.
         <a class="mw-redirect" href="/wiki/Bibcode_(identifier)" title="Bibcode (identifier)">
          Bibcode
         </a>
         :
         <a class="external text" href="https://ui.adsabs.harvard.edu/abs/1974Sci...185.1124T" rel="nofollow">
          1974Sci...185.1124T
         </a>
         .
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1126%2Fscience.185.4157.1124" rel="nofollow">
          10.1126/science.185.4157.1124
         </a>
         .
         <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">
          PMID
         </a>
         <a class="external text" href="https://pubmed.ncbi.nlm.nih.gov/17835457" rel="nofollow">
          17835457
         </a>
         .
         <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">
          S2CID
         </a>
         <a class="external text" href="https://api.semanticscholar.org/CorpusID:143452957" rel="nofollow">
          143452957
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Judgment+under+Uncertainty%3A+Heuristics+and+Biases&amp;rft.volume=185&amp;rft.issue=4157&amp;rft.pages=1124-1131&amp;rft.date=1974&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.185.4157.1124&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A143452957%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F17835457&amp;rft_id=info%3Abibcode%2F1974Sci...185.1124T&amp;rft.aulast=Tversky&amp;rft.aufirst=A.&amp;rft.au=Kahneman%2C+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-Wickens2015-10">
       <span class="mw-cite-backlink">
        ^
        <a href="#cite_ref-Wickens2015_10-0">
         <sup>
          <i>
           <b>
            a
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-Wickens2015_10-1">
         <sup>
          <i>
           <b>
            b
           </b>
          </i>
         </sup>
        </a>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation book cs1" id="CITEREFWickensHollandsBanburyParasuraman2015">
         Wickens, Christopher D.; Hollands, Justin G.; Banbury, Simon; Parasuraman, Raja (2015).
         <a class="external text" href="https://books.google.com/books?id=_rFmCgAAQBAJ" rel="nofollow">
          <i>
           Engineering Psychology and Human Performance
          </i>
         </a>
         (4th ed.). Psychology Press. pp. 335–338.
         <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">
          ISBN
         </a>
         <a href="/wiki/Special:BookSources/9781317351320" title="Special:BookSources/9781317351320">
          <bdi>
           9781317351320
          </bdi>
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Engineering+Psychology+and+Human+Performance&amp;rft.pages=335-338&amp;rft.edition=4th&amp;rft.pub=Psychology+Press&amp;rft.date=2015&amp;rft.isbn=9781317351320&amp;rft.aulast=Wickens&amp;rft.aufirst=Christopher+D.&amp;rft.au=Hollands%2C+Justin+G.&amp;rft.au=Banbury%2C+Simon&amp;rft.au=Parasuraman%2C+Raja&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3D_rFmCgAAQBAJ&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-Sagepub-11">
       <span class="mw-cite-backlink">
        ^
        <a href="#cite_ref-Sagepub_11-0">
         <sup>
          <i>
           <b>
            a
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-Sagepub_11-1">
         <sup>
          <i>
           <b>
            b
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-Sagepub_11-2">
         <sup>
          <i>
           <b>
            c
           </b>
          </i>
         </sup>
        </a>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFMosierDunbarMcDonnellSkitka1998">
         Mosier, Kathleen L.; Dunbar, Melisa; McDonnell, Lori; Skitka, Linda J.; Burdick, Mark; Rosenblatt, Bonnie (1998). "Automation Bias and Errors: Are Teams Better than Individuals?".
         <i>
          Proceedings of the Human Factors and Ergonomics Society Annual Meeting
         </i>
         .
         <b>
          42
         </b>
         (3): 201–205.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1177%2F154193129804200304" rel="nofollow">
          10.1177/154193129804200304
         </a>
         .
         <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">
          S2CID
         </a>
         <a class="external text" href="https://api.semanticscholar.org/CorpusID:62603631" rel="nofollow">
          62603631
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+Human+Factors+and+Ergonomics+Society+Annual+Meeting&amp;rft.atitle=Automation+Bias+and+Errors%3A+Are+Teams+Better+than+Individuals%3F&amp;rft.volume=42&amp;rft.issue=3&amp;rft.pages=201-205&amp;rft.date=1998&amp;rft_id=info%3Adoi%2F10.1177%2F154193129804200304&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A62603631%23id-name%3DS2CID&amp;rft.aulast=Mosier&amp;rft.aufirst=Kathleen+L.&amp;rft.au=Dunbar%2C+Melisa&amp;rft.au=McDonnell%2C+Lori&amp;rft.au=Skitka%2C+Linda+J.&amp;rft.au=Burdick%2C+Mark&amp;rft.au=Rosenblatt%2C+Bonnie&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-The_Journal_of_the_Human_Factors_and_Ergonomics_Society-12">
       <span class="mw-cite-backlink">
        ^
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-0">
         <sup>
          <i>
           <b>
            a
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-1">
         <sup>
          <i>
           <b>
            b
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-2">
         <sup>
          <i>
           <b>
            c
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-3">
         <sup>
          <i>
           <b>
            d
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-4">
         <sup>
          <i>
           <b>
            e
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-5">
         <sup>
          <i>
           <b>
            f
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-6">
         <sup>
          <i>
           <b>
            g
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-7">
         <sup>
          <i>
           <b>
            h
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-8">
         <sup>
          <i>
           <b>
            i
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-9">
         <sup>
          <i>
           <b>
            j
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-10">
         <sup>
          <i>
           <b>
            k
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-11">
         <sup>
          <i>
           <b>
            l
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-12">
         <sup>
          <i>
           <b>
            m
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-13">
         <sup>
          <i>
           <b>
            n
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-14">
         <sup>
          <i>
           <b>
            o
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_Journal_of_the_Human_Factors_and_Ergonomics_Society_12-15">
         <sup>
          <i>
           <b>
            p
           </b>
          </i>
         </sup>
        </a>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFParasuramanManzey2010">
         Parasuraman, Raja; Manzey, Dietrich (June 2010).
         <a class="external text" href="https://www.researchgate.net/publication/47792928" rel="nofollow">
          "Complacency and Bias in Human Use of Automation: An Attentional Integration"
         </a>
         .
         <i>
          The Journal of the Human Factors and Ergonomics Society
         </i>
         .
         <b>
          52
         </b>
         (3): 381–410.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1177%2F0018720810376055" rel="nofollow">
          10.1177/0018720810376055
         </a>
         .
         <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">
          PMID
         </a>
         <a class="external text" href="https://pubmed.ncbi.nlm.nih.gov/21077562" rel="nofollow">
          21077562
         </a>
         .
         <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">
          S2CID
         </a>
         <a class="external text" href="https://api.semanticscholar.org/CorpusID:2279803" rel="nofollow">
          2279803
         </a>
         <span class="reference-accessdate">
          . Retrieved
          <span class="nowrap">
           17 January
          </span>
          2017
         </span>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Journal+of+the+Human+Factors+and+Ergonomics+Society&amp;rft.atitle=Complacency+and+Bias+in+Human+Use+of+Automation%3A+An+Attentional+Integration&amp;rft.volume=52&amp;rft.issue=3&amp;rft.pages=381-410&amp;rft.date=2010-06&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A2279803%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F21077562&amp;rft_id=info%3Adoi%2F10.1177%2F0018720810376055&amp;rft.aulast=Parasuraman&amp;rft.aufirst=Raja&amp;rft.au=Manzey%2C+Dietrich&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F47792928&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-goddard2012-13">
       <span class="mw-cite-backlink">
        ^
        <a href="#cite_ref-goddard2012_13-0">
         <sup>
          <i>
           <b>
            a
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-goddard2012_13-1">
         <sup>
          <i>
           <b>
            b
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-goddard2012_13-2">
         <sup>
          <i>
           <b>
            c
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-goddard2012_13-3">
         <sup>
          <i>
           <b>
            d
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-goddard2012_13-4">
         <sup>
          <i>
           <b>
            e
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-goddard2012_13-5">
         <sup>
          <i>
           <b>
            f
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-goddard2012_13-6">
         <sup>
          <i>
           <b>
            g
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-goddard2012_13-7">
         <sup>
          <i>
           <b>
            h
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-goddard2012_13-8">
         <sup>
          <i>
           <b>
            i
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-goddard2012_13-9">
         <sup>
          <i>
           <b>
            j
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-goddard2012_13-10">
         <sup>
          <i>
           <b>
            k
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-goddard2012_13-11">
         <sup>
          <i>
           <b>
            l
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-goddard2012_13-12">
         <sup>
          <i>
           <b>
            m
           </b>
          </i>
         </sup>
        </a>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFGoddardRoudsariWyatt2012">
         <a class="mw-redirect" href="/wiki/Katrina_A._B._Goddard" title="Katrina A. B. Goddard">
          Goddard, K.
         </a>
         ; Roudsari, A.; Wyatt, J. C. (2012).
         <a class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3240751" rel="nofollow">
          "Automation bias: a systematic review of frequency, effect mediators, and mitigators"
         </a>
         .
         <i>
          Journal of the American Medical Informatics Association
         </i>
         .
         <b>
          19
         </b>
         (1): 121–127.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1136%2Famiajnl-2011-000089" rel="nofollow">
          10.1136/amiajnl-2011-000089
         </a>
         .
         <a class="mw-redirect" href="/wiki/PMC_(identifier)" title="PMC (identifier)">
          PMC
         </a>
         <span class="id-lock-free" title="Freely accessible">
          <a class="external text" href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC3240751" rel="nofollow">
           3240751
          </a>
         </span>
         .
         <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">
          PMID
         </a>
         <a class="external text" href="https://pubmed.ncbi.nlm.nih.gov/21685142" rel="nofollow">
          21685142
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+American+Medical+Informatics+Association&amp;rft.atitle=Automation+bias%3A+a+systematic+review+of+frequency%2C+effect+mediators%2C+and+mitigators&amp;rft.volume=19&amp;rft.issue=1&amp;rft.pages=121-127&amp;rft.date=2012&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3240751%23id-name%3DPMC&amp;rft_id=info%3Apmid%2F21685142&amp;rft_id=info%3Adoi%2F10.1136%2Famiajnl-2011-000089&amp;rft.aulast=Goddard&amp;rft.aufirst=K.&amp;rft.au=Roudsari%2C+A.&amp;rft.au=Wyatt%2C+J.+C.&amp;rft_id=https%3A%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC3240751&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-Alberdi2009-14">
       <span class="mw-cite-backlink">
        ^
        <a href="#cite_ref-Alberdi2009_14-0">
         <sup>
          <i>
           <b>
            a
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-Alberdi2009_14-1">
         <sup>
          <i>
           <b>
            b
           </b>
          </i>
         </sup>
        </a>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation book cs1" id="CITEREFAlberdiStriginiPovyakaloAyton2009">
         Alberdi, Eugenio; Strigini, Lorenzo; Povyakalo, Andrey A.; Ayton, Peter (2009).
         <a class="external text" href="http://openaccess.city.ac.uk/384/2/Alberdi-et-alSAFECOMP09.pdf" rel="nofollow">
          "Why Are People's Decisions Sometimes Worse with Computer Support?"
         </a>
         <span class="cs1-format">
          (PDF)
         </span>
         .
         <i>
          Computer Safety, Reliability, and Security
         </i>
         . Lecture Notes in Computer Science. Vol. 5775. Springer Berlin Heidelberg. pp. 18–31.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1007%2F978-3-642-04468-7_3" rel="nofollow">
          10.1007/978-3-642-04468-7_3
         </a>
         .
         <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">
          ISBN
         </a>
         <a href="/wiki/Special:BookSources/978-3-642-04467-0" title="Special:BookSources/978-3-642-04467-0">
          <bdi>
           978-3-642-04467-0
          </bdi>
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Why+Are+People%27s+Decisions+Sometimes+Worse+with+Computer+Support%3F&amp;rft.btitle=Computer+Safety%2C+Reliability%2C+and+Security&amp;rft.series=Lecture+Notes+in+Computer+Science&amp;rft.pages=18-31&amp;rft.pub=Springer+Berlin+Heidelberg&amp;rft.date=2009&amp;rft_id=info%3Adoi%2F10.1007%2F978-3-642-04468-7_3&amp;rft.isbn=978-3-642-04467-0&amp;rft.aulast=Alberdi&amp;rft.aufirst=Eugenio&amp;rft.au=Strigini%2C+Lorenzo&amp;rft.au=Povyakalo%2C+Andrey+A.&amp;rft.au=Ayton%2C+Peter&amp;rft_id=http%3A%2F%2Fopenaccess.city.ac.uk%2F384%2F2%2FAlberdi-et-alSAFECOMP09.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-goddard2014-15">
       <span class="mw-cite-backlink">
        <b>
         <a href="#cite_ref-goddard2014_15-0">
          ^
         </a>
        </b>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFGoddardRoudsariWyatt2014">
         Goddard, Kate; Roudsari, Abdul; Wyatt, Jeremy C. (2014). "Automation bias: Empirical results assessing influencing factors".
         <i>
          International Journal of Medical Informatics
         </i>
         .
         <b>
          83
         </b>
         (5): 368–375.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1016%2Fj.ijmedinf.2014.01.001" rel="nofollow">
          10.1016/j.ijmedinf.2014.01.001
         </a>
         .
         <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">
          PMID
         </a>
         <a class="external text" href="https://pubmed.ncbi.nlm.nih.gov/24581700" rel="nofollow">
          24581700
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Medical+Informatics&amp;rft.atitle=Automation+bias%3A+Empirical+results+assessing+influencing+factors&amp;rft.volume=83&amp;rft.issue=5&amp;rft.pages=368-375&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ijmedinf.2014.01.001&amp;rft_id=info%3Apmid%2F24581700&amp;rft.aulast=Goddard&amp;rft.aufirst=Kate&amp;rft.au=Roudsari%2C+Abdul&amp;rft.au=Wyatt%2C+Jeremy+C.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-The_International_Journal_of_Aviation_Psychology-16">
       <span class="mw-cite-backlink">
        ^
        <a href="#cite_ref-The_International_Journal_of_Aviation_Psychology_16-0">
         <sup>
          <i>
           <b>
            a
           </b>
          </i>
         </sup>
        </a>
        <a href="#cite_ref-The_International_Journal_of_Aviation_Psychology_16-1">
         <sup>
          <i>
           <b>
            b
           </b>
          </i>
         </sup>
        </a>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFMosierSkitkaDunbarMcDonnell2009">
         Mosier, Kathleen; Skitka, Linda; Dunbar, Melisa; McDonnell, Lori (November 13, 2009). "Aircrews and Automation Bias: The Advantages of Teamwork?".
         <i>
          The International Journal of Aviation Psychology
         </i>
         .
         <b>
          11
         </b>
         (1): 1–14.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1207%2FS15327108IJAP1101_1" rel="nofollow">
          10.1207/S15327108IJAP1101_1
         </a>
         .
         <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">
          S2CID
         </a>
         <a class="external text" href="https://api.semanticscholar.org/CorpusID:4132245" rel="nofollow">
          4132245
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+International+Journal+of+Aviation+Psychology&amp;rft.atitle=Aircrews+and+Automation+Bias%3A+The+Advantages+of+Teamwork%3F&amp;rft.volume=11&amp;rft.issue=1&amp;rft.pages=1-14&amp;rft.date=2009-11-13&amp;rft_id=info%3Adoi%2F10.1207%2FS15327108IJAP1101_1&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A4132245%23id-name%3DS2CID&amp;rft.aulast=Mosier&amp;rft.aufirst=Kathleen&amp;rft.au=Skitka%2C+Linda&amp;rft.au=Dunbar%2C+Melisa&amp;rft.au=McDonnell%2C+Lori&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-17">
       <span class="mw-cite-backlink">
        <b>
         <a href="#cite_ref-17">
          ^
         </a>
        </b>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFEndsley2017">
         Endsley, Mica (2017).
         <a class="external text" href="https://doi.org/10.1177%2F0018720816681350" rel="nofollow">
          "From Here to Autonomy Lessons Learned From Human–Automation Research"
         </a>
         .
         <i>
          Human Factors
         </i>
         .
         <b>
          59
         </b>
         (1): 5–27.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <span class="id-lock-free" title="Freely accessible">
          <a class="external text" href="https://doi.org/10.1177%2F0018720816681350" rel="nofollow">
           10.1177/0018720816681350
          </a>
         </span>
         .
         <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">
          PMID
         </a>
         <a class="external text" href="https://pubmed.ncbi.nlm.nih.gov/28146676" rel="nofollow">
          28146676
         </a>
         .
         <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">
          S2CID
         </a>
         <a class="external text" href="https://api.semanticscholar.org/CorpusID:3771328" rel="nofollow">
          3771328
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Human+Factors&amp;rft.atitle=From+Here+to+Autonomy+Lessons+Learned+From+Human%E2%80%93Automation+Research&amp;rft.volume=59&amp;rft.issue=1&amp;rft.pages=5-27&amp;rft.date=2017&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A3771328%23id-name%3DS2CID&amp;rft_id=info%3Apmid%2F28146676&amp;rft_id=info%3Adoi%2F10.1177%2F0018720816681350&amp;rft.aulast=Endsley&amp;rft.aufirst=Mica&amp;rft_id=https%3A%2F%2Fdoi.org%2F10.1177%252F0018720816681350&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-18">
       <span class="mw-cite-backlink">
        <b>
         <a href="#cite_ref-18">
          ^
         </a>
        </b>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFBainbridge1983">
         <a class="mw-redirect" href="/wiki/Lisanne_Bainbridge" title="Lisanne Bainbridge">
          Bainbridge, Lisanne
         </a>
         (1983).
         <a class="external text" href="https://web.archive.org/web/20200913080929/https://www.ise.ncsu.edu/wp-content/uploads/2017/02/Bainbridge_1983_Automatica.pdf" rel="nofollow">
          "Ironies of automation"
         </a>
         <span class="cs1-format">
          (PDF)
         </span>
         .
         <i>
          Automatica
         </i>
         .
         <b>
          19
         </b>
         (6): 775–779.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1016%2F0005-1098%2883%2990046-8" rel="nofollow">
          10.1016/0005-1098(83)90046-8
         </a>
         .
         <a class="mw-redirect" href="/wiki/S2CID_(identifier)" title="S2CID (identifier)">
          S2CID
         </a>
         <a class="external text" href="https://api.semanticscholar.org/CorpusID:12667742" rel="nofollow">
          12667742
         </a>
         . Archived from
         <a class="external text" href="https://www.ise.ncsu.edu/wp-content/uploads/2017/02/Bainbridge_1983_Automatica.pdf" rel="nofollow">
          the original
         </a>
         <span class="cs1-format">
          (PDF)
         </span>
         on 2020-09-13
         <span class="reference-accessdate">
          . Retrieved
          <span class="nowrap">
           2019-12-07
          </span>
         </span>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Automatica&amp;rft.atitle=Ironies+of+automation&amp;rft.volume=19&amp;rft.issue=6&amp;rft.pages=775-779&amp;rft.date=1983&amp;rft_id=info%3Adoi%2F10.1016%2F0005-1098%2883%2990046-8&amp;rft_id=https%3A%2F%2Fapi.semanticscholar.org%2FCorpusID%3A12667742%23id-name%3DS2CID&amp;rft.aulast=Bainbridge&amp;rft.aufirst=Lisanne&amp;rft_id=https%3A%2F%2Fwww.ise.ncsu.edu%2Fwp-content%2Fuploads%2F2017%2F02%2FBainbridge_1983_Automatica.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-ParasuramanMolloy1993-19">
       <span class="mw-cite-backlink">
        <b>
         <a href="#cite_ref-ParasuramanMolloy1993_19-0">
          ^
         </a>
        </b>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFParasuramanMolloySingh1993">
         Parasuraman, Raja; Molloy, Robert; Singh, Indramani L. (1993). "Performance Consequences of Automation-Induced 'Complacency'
         <span class="cs1-kern-right">
         </span>
         ".
         <i>
          The International Journal of Aviation Psychology
         </i>
         .
         <b>
          3
         </b>
         : 1–23.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1207%2Fs15327108ijap0301_1" rel="nofollow">
          10.1207/s15327108ijap0301_1
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+International+Journal+of+Aviation+Psychology&amp;rft.atitle=Performance+Consequences+of+Automation-Induced+%27Complacency%27&amp;rft.volume=3&amp;rft.pages=1-23&amp;rft.date=1993&amp;rft_id=info%3Adoi%2F10.1207%2Fs15327108ijap0301_1&amp;rft.aulast=Parasuraman&amp;rft.aufirst=Raja&amp;rft.au=Molloy%2C+Robert&amp;rft.au=Singh%2C+Indramani+L.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-20">
       <span class="mw-cite-backlink">
        <b>
         <a href="#cite_ref-20">
          ^
         </a>
        </b>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation web cs1">
         <a class="external text" href="https://www.ntsb.gov/investigations/AccidentReports/Pages/HAR1903.aspx" rel="nofollow">
          "Collision Between Vehicle Controlled by Developmental Automated Driving System and Pedestrian"
         </a>
         .
         <i>
          www.ntsb.gov
         </i>
         <span class="reference-accessdate">
          . Retrieved
          <span class="nowrap">
           December 19,
          </span>
          2019
         </span>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.ntsb.gov&amp;rft.atitle=Collision+Between+Vehicle+Controlled+by+Developmental+Automated+Driving+System+and+Pedestrian&amp;rft_id=https%3A%2F%2Fwww.ntsb.gov%2Finvestigations%2FAccidentReports%2FPages%2FHAR1903.aspx&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
      <li id="cite_note-bahner2008-21">
       <span class="mw-cite-backlink">
        <b>
         <a href="#cite_ref-bahner2008_21-0">
          ^
         </a>
        </b>
       </span>
       <span class="reference-text">
        <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
        <cite class="citation journal cs1" id="CITEREFBahnerHüperManzey2008">
         Bahner, J. Elin; Hüper, Anke-Dorothea; Manzey, Dietrich (2008). "Misuse of automated decision aids: Complacency, automation bias and the impact of training experience".
         <i>
          International Journal of Human-Computer Studies
         </i>
         .
         <b>
          66
         </b>
         (9): 688–699.
         <a class="mw-redirect" href="/wiki/Doi_(identifier)" title="Doi (identifier)">
          doi
         </a>
         :
         <a class="external text" href="https://doi.org/10.1016%2Fj.ijhcs.2008.06.001" rel="nofollow">
          10.1016/j.ijhcs.2008.06.001
         </a>
         .
        </cite>
        <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Human-Computer+Studies&amp;rft.atitle=Misuse+of+automated+decision+aids%3A+Complacency%2C+automation+bias+and+the+impact+of+training+experience&amp;rft.volume=66&amp;rft.issue=9&amp;rft.pages=688-699&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.1016%2Fj.ijhcs.2008.06.001&amp;rft.aulast=Bahner&amp;rft.aufirst=J.+Elin&amp;rft.au=H%C3%BCper%2C+Anke-Dorothea&amp;rft.au=Manzey%2C+Dietrich&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
        </span>
       </span>
      </li>
     </ol>
    </div>
    <div class="mw-heading mw-heading2">
     <h2 id="Further_reading">
      Further reading
     </h2>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=22" title="Edit section: Further reading">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <ul>
     <li>
      <link href="mw-data:TemplateStyles:r1238218222" rel="mw-deduplicated-inline-style"/>
      <cite class="citation book cs1" id="CITEREFGoddardRoudsariWyatt2011">
       <a class="mw-redirect" href="/wiki/Katrina_A._B._Goddard" title="Katrina A. B. Goddard">
        Goddard, K
       </a>
       ; Roudsari, A; Wyatt, J. C. (2011).
       <a class="external text" href="https://books.google.com/books?id=NsbaN_fXRe4C&amp;pg=PA17" rel="nofollow">
        "Automation bias - a hidden issue for clinical decision support system use"
       </a>
       .
       <i>
        International Perspectives in Health Informatics
       </i>
       . Studies in Health Technology and Informatics. Vol. 164. pp. 17–22.
       <a class="mw-redirect" href="/wiki/ISBN_(identifier)" title="ISBN (identifier)">
        ISBN
       </a>
       <a href="/wiki/Special:BookSources/978-1-60750-708-6" title="Special:BookSources/978-1-60750-708-6">
        <bdi>
         978-1-60750-708-6
        </bdi>
       </a>
       .
       <a class="mw-redirect" href="/wiki/PMID_(identifier)" title="PMID (identifier)">
        PMID
       </a>
       <a class="external text" href="https://pubmed.ncbi.nlm.nih.gov/21335682" rel="nofollow">
        21335682
       </a>
       .
      </cite>
      <span class="Z3988" title="ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Automation+bias+-+a+hidden+issue+for+clinical+decision+support+system+use&amp;rft.btitle=International+Perspectives+in+Health+Informatics&amp;rft.series=Studies+in+Health+Technology+and+Informatics&amp;rft.pages=17-22&amp;rft.date=2011&amp;rft_id=info%3Apmid%2F21335682&amp;rft.isbn=978-1-60750-708-6&amp;rft.aulast=Goddard&amp;rft.aufirst=K&amp;rft.au=Roudsari%2C+A&amp;rft.au=Wyatt%2C+J.+C.&amp;rft_id=https%3A%2F%2Fbooks.google.com%2Fbooks%3Fid%3DNsbaN_fXRe4C%26pg%3DPA17&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AAutomation+bias">
      </span>
     </li>
    </ul>
    <div class="mw-heading mw-heading2">
     <h2 id="External_links">
      External links
     </h2>
     <span class="mw-editsection">
      <span class="mw-editsection-bracket">
       [
      </span>
      <a href="/w/index.php?title=Automation_bias&amp;action=edit&amp;section=23" title="Edit section: External links">
       <span>
        edit
       </span>
      </a>
      <span class="mw-editsection-bracket">
       ]
      </span>
     </span>
    </div>
    <ul>
     <li>
      <a class="external text" href="https://www.forbes.com/sites/brucekasanoff/2017/03/29/sorry-you-cant-make-a-logical-data-driven-decision-without-intuition/" rel="nofollow">
       <i>
        175 Reasons Why You Don't Think Clearly
       </i>
      </a>
     </li>
    </ul>
    <div class="navbox-styles">
     <link href="mw-data:TemplateStyles:r1129693374" rel="mw-deduplicated-inline-style"/>
     <style data-mw-deduplicate="TemplateStyles:r1236075235">
      .mw-parser-output .navbox{box-sizing:border-box;border:1px solid #a2a9b1;width:100%;clear:both;font-size:88%;text-align:center;padding:1px;margin:1em auto 0}.mw-parser-output .navbox .navbox{margin-top:0}.mw-parser-output .navbox+.navbox,.mw-parser-output .navbox+.navbox-styles+.navbox{margin-top:-1px}.mw-parser-output .navbox-inner,.mw-parser-output .navbox-subgroup{width:100%}.mw-parser-output .navbox-group,.mw-parser-output .navbox-title,.mw-parser-output .navbox-abovebelow{padding:0.25em 1em;line-height:1.5em;text-align:center}.mw-parser-output .navbox-group{white-space:nowrap;text-align:right}.mw-parser-output .navbox,.mw-parser-output .navbox-subgroup{background-color:#fdfdfd}.mw-parser-output .navbox-list{line-height:1.5em;border-color:#fdfdfd}.mw-parser-output .navbox-list-with-group{text-align:left;border-left-width:2px;border-left-style:solid}.mw-parser-output tr+tr>.navbox-abovebelow,.mw-parser-output tr+tr>.navbox-group,.mw-parser-output tr+tr>.navbox-image,.mw-parser-output tr+tr>.navbox-list{border-top:2px solid #fdfdfd}.mw-parser-output .navbox-title{background-color:#ccf}.mw-parser-output .navbox-abovebelow,.mw-parser-output .navbox-group,.mw-parser-output .navbox-subgroup .navbox-title{background-color:#ddf}.mw-parser-output .navbox-subgroup .navbox-group,.mw-parser-output .navbox-subgroup .navbox-abovebelow{background-color:#e6e6ff}.mw-parser-output .navbox-even{background-color:#f7f7f7}.mw-parser-output .navbox-odd{background-color:transparent}.mw-parser-output .navbox .hlist td dl,.mw-parser-output .navbox .hlist td ol,.mw-parser-output .navbox .hlist td ul,.mw-parser-output .navbox td.hlist dl,.mw-parser-output .navbox td.hlist ol,.mw-parser-output .navbox td.hlist ul{padding:0.125em 0}.mw-parser-output .navbox .navbar{display:block;font-size:100%}.mw-parser-output .navbox-title .navbar{float:left;text-align:left;margin-right:0.5em}body.skin--responsive .mw-parser-output .navbox-image img{max-width:none!important}@media print{body.ns-0 .mw-parser-output .navbox{display:none!important}}
     </style>
    </div>
    <div aria-labelledby="Biases" class="navbox" role="navigation" style="padding:3px">
     <table class="nowraplinks mw-collapsible autocollapse navbox-inner" style="border-spacing:0;background:transparent;color:inherit">
      <tbody>
       <tr>
        <th class="navbox-title" colspan="2" scope="col">
         <link href="mw-data:TemplateStyles:r1129693374" rel="mw-deduplicated-inline-style"/>
         <link href="mw-data:TemplateStyles:r1239400231" rel="mw-deduplicated-inline-style"/>
         <div class="navbar plainlinks hlist navbar-mini">
          <ul>
           <li class="nv-view">
            <a href="/wiki/Template:Biases" title="Template:Biases">
             <abbr title="View this template">
              v
             </abbr>
            </a>
           </li>
           <li class="nv-talk">
            <a href="/wiki/Template_talk:Biases" title="Template talk:Biases">
             <abbr title="Discuss this template">
              t
             </abbr>
            </a>
           </li>
           <li class="nv-edit">
            <a href="/wiki/Special:EditPage/Template:Biases" title="Special:EditPage/Template:Biases">
             <abbr title="Edit this template">
              e
             </abbr>
            </a>
           </li>
          </ul>
         </div>
         <div id="Biases" style="font-size:114%;margin:0 4em">
          <a href="/wiki/Bias" title="Bias">
           Biases
          </a>
         </div>
        </th>
       </tr>
       <tr>
        <th class="navbox-group" scope="row" style="width:1%">
         <div style="display: inline-block; line-height: 1.2em; padding: .1em 0;">
          <a href="/wiki/Cognitive_bias" title="Cognitive bias">
           Cognitive biases
          </a>
         </div>
        </th>
        <td class="navbox-list-with-group navbox-list navbox-odd hlist" style="width:100%;padding:0">
         <div style="padding:0 0.25em">
          <ul>
           <li>
            <a href="/wiki/Acquiescence_bias" title="Acquiescence bias">
             Acquiescence
            </a>
           </li>
           <li>
            <a href="/wiki/Ambiguity_effect" title="Ambiguity effect">
             Ambiguity
            </a>
           </li>
           <li>
            <a href="/wiki/Affinity_bias" title="Affinity bias">
             Affinity
            </a>
           </li>
           <li>
            <a class="mw-redirect" href="/wiki/Anchoring_(cognitive_bias)" title="Anchoring (cognitive bias)">
             Anchoring
            </a>
           </li>
           <li>
            <a href="/wiki/Attentional_bias" title="Attentional bias">
             Attentional
            </a>
           </li>
           <li>
            <a href="/wiki/Attribution_bias" title="Attribution bias">
             Attribution
            </a>
            <ul>
             <li>
              <a href="/wiki/Actor%E2%80%93observer_asymmetry" title="Actor–observer asymmetry">
               Actor–observer
              </a>
             </li>
             <li>
              <a href="/wiki/Fundamental_attribution_error" title="Fundamental attribution error">
               Correspondence
              </a>
             </li>
            </ul>
           </li>
           <li>
            <a href="/wiki/Authority_bias" title="Authority bias">
             Authority
            </a>
           </li>
           <li>
            <a class="mw-selflink selflink">
             Automation
            </a>
           </li>
           <li>
            <a href="/wiki/Availability_heuristic" title="Availability heuristic">
             Availability
            </a>
            <ul>
             <li>
              <a href="/wiki/Mean_world_syndrome" title="Mean world syndrome">
               Mean world
              </a>
             </li>
            </ul>
           </li>
           <li>
            <a href="/wiki/Belief_bias" title="Belief bias">
             Belief
            </a>
           </li>
           <li>
            <a href="/wiki/Bias_blind_spot" title="Bias blind spot">
             Blind spot
            </a>
           </li>
           <li>
            <a href="/wiki/Choice-supportive_bias" title="Choice-supportive bias">
             Choice-supportive
            </a>
           </li>
           <li>
            <a href="/wiki/Escalation_of_commitment" title="Escalation of commitment">
             Commitment
            </a>
           </li>
           <li>
            <a href="/wiki/Confirmation_bias" title="Confirmation bias">
             Confirmation
            </a>
            <ul>
             <li>
              <a href="/wiki/Selective_perception" title="Selective perception">
               Selective perception
              </a>
             </li>
            </ul>
           </li>
           <li>
            <a href="/wiki/Compassion_fade" title="Compassion fade">
             Compassion fade
            </a>
           </li>
           <li>
            <a href="/wiki/Congruence_bias" title="Congruence bias">
             Congruence
            </a>
           </li>
           <li>
            <a href="/wiki/Cultural_bias" title="Cultural bias">
             Cultural
            </a>
           </li>
           <li>
            <a href="/wiki/Declinism" title="Declinism">
             Declinism
            </a>
           </li>
           <li>
            <a href="/wiki/Distinction_bias" title="Distinction bias">
             Distinction
            </a>
           </li>
           <li>
            <a href="/wiki/Dunning%E2%80%93Kruger_effect" title="Dunning–Kruger effect">
             Dunning–Kruger
            </a>
           </li>
           <li>
            <a href="/wiki/Egocentric_bias" title="Egocentric bias">
             Egocentric
            </a>
            <ul>
             <li>
              <a href="/wiki/Curse_of_knowledge" title="Curse of knowledge">
               Curse of knowledge
              </a>
             </li>
            </ul>
           </li>
           <li>
            <a href="/wiki/Emotional_bias" title="Emotional bias">
             Emotional
            </a>
           </li>
           <li>
            <a href="/wiki/Extrinsic_incentives_bias" title="Extrinsic incentives bias">
             Extrinsic incentives
            </a>
           </li>
           <li>
            <a href="/wiki/Fading_affect_bias" title="Fading affect bias">
             Fading affect
            </a>
           </li>
           <li>
            <a href="/wiki/Framing_effect_(psychology)" title="Framing effect (psychology)">
             Framing
            </a>
           </li>
           <li>
            <a href="/wiki/Frequency_illusion" title="Frequency illusion">
             Frequency
            </a>
           </li>
           <li>
            <a href="/wiki/Frog_pond_effect" title="Frog pond effect">
             Frog pond effect
            </a>
           </li>
           <li>
            <a href="/wiki/Halo_effect" title="Halo effect">
             Halo effect
            </a>
           </li>
           <li>
            <a href="/wiki/Hindsight_bias" title="Hindsight bias">
             Hindsight
            </a>
           </li>
           <li>
            <a href="/wiki/Horn_effect" title="Horn effect">
             Horn effect
            </a>
           </li>
           <li>
            <a href="/wiki/Hostile_attribution_bias" title="Hostile attribution bias">
             Hostile attribution
            </a>
           </li>
           <li>
            <a href="/wiki/Impact_bias" title="Impact bias">
             Impact
            </a>
           </li>
           <li>
            <a href="/wiki/Implicit_stereotype" title="Implicit stereotype">
             Implicit
            </a>
           </li>
           <li>
            <a href="/wiki/In-group_favoritism" title="In-group favoritism">
             In-group
            </a>
           </li>
           <li>
            <a href="/wiki/Intentionality_bias" title="Intentionality bias">
             Intentionality
            </a>
           </li>
           <li>
            <a href="/wiki/Illusion_of_transparency" title="Illusion of transparency">
             Illusion of transparency
            </a>
           </li>
           <li>
            <a href="/wiki/Mean_world_syndrome" title="Mean world syndrome">
             Mean world syndrome
            </a>
           </li>
           <li>
            <a href="/wiki/Mere-exposure_effect" title="Mere-exposure effect">
             Mere-exposure effect
            </a>
           </li>
           <li>
            <a href="/wiki/Narrative_bias" title="Narrative bias">
             Narrative
            </a>
           </li>
           <li>
            <a href="/wiki/Negativity_bias" title="Negativity bias">
             Negativity
            </a>
           </li>
           <li>
            <a href="/wiki/Normalcy_bias" title="Normalcy bias">
             Normalcy
            </a>
           </li>
           <li>
            <a href="/wiki/Omission_bias" title="Omission bias">
             Omission
            </a>
           </li>
           <li>
            <a href="/wiki/Optimism_bias" title="Optimism bias">
             Optimism
            </a>
           </li>
           <li>
            <a href="/wiki/Out-group_homogeneity" title="Out-group homogeneity">
             Out-group homogeneity
            </a>
           </li>
           <li>
            <a href="/wiki/Outcome_bias" title="Outcome bias">
             Outcome
            </a>
           </li>
           <li>
            <a href="/wiki/Overton_window" title="Overton window">
             Overton window
            </a>
           </li>
           <li>
            <a href="/wiki/Precision_bias" title="Precision bias">
             Precision
            </a>
           </li>
           <li>
            <a href="/wiki/Present_bias" title="Present bias">
             Present
            </a>
           </li>
           <li>
            <a href="/wiki/Pro-innovation_bias" title="Pro-innovation bias">
             Pro-innovation
            </a>
           </li>
           <li>
            <a href="/wiki/Proximity_bias" title="Proximity bias">
             Proximity
            </a>
           </li>
           <li>
            <a href="/wiki/Response_bias" title="Response bias">
             Response
            </a>
           </li>
           <li>
            <a href="/wiki/Restraint_bias" title="Restraint bias">
             Restraint
            </a>
           </li>
           <li>
            <a href="/wiki/Self-serving_bias" title="Self-serving bias">
             Self-serving
            </a>
           </li>
           <li>
            <a href="/wiki/Social_comparison_bias" title="Social comparison bias">
             Social comparison
            </a>
           </li>
           <li>
            <a href="/wiki/Social_influence_bias" title="Social influence bias">
             Social influence bias
            </a>
           </li>
           <li>
            <a href="/wiki/Spotlight_effect" title="Spotlight effect">
             Spotlight
            </a>
           </li>
           <li>
            <a href="/wiki/Status_quo_bias" title="Status quo bias">
             Status quo
            </a>
           </li>
           <li>
            <a href="/wiki/Attribute_substitution" title="Attribute substitution">
             Substitution
            </a>
           </li>
           <li>
            <a href="/wiki/Time-saving_bias" title="Time-saving bias">
             Time-saving
            </a>
           </li>
           <li>
            <a href="/wiki/Trait_ascription_bias" title="Trait ascription bias">
             Trait ascription
            </a>
           </li>
           <li>
            <a href="/wiki/Turkey_illusion" title="Turkey illusion">
             Turkey illusion
            </a>
           </li>
           <li>
            <a href="/wiki/Von_Restorff_effect" title="Von Restorff effect">
             von Restorff effect
            </a>
           </li>
           <li>
            <a href="/wiki/Zero-risk_bias" title="Zero-risk bias">
             Zero-risk
            </a>
           </li>
           <li>
            <a href="/wiki/Cognitive_bias_in_animals" title="Cognitive bias in animals">
             In animals
            </a>
           </li>
          </ul>
         </div>
        </td>
       </tr>
       <tr>
        <th class="navbox-group" scope="row" style="width:1%">
         <a href="/wiki/Bias_(statistics)" title="Bias (statistics)">
          Statistical biases
         </a>
        </th>
        <td class="navbox-list-with-group navbox-list navbox-even hlist" style="width:100%;padding:0">
         <div style="padding:0 0.25em">
          <ul>
           <li>
            <a href="/wiki/Bias_of_an_estimator" title="Bias of an estimator">
             Estimator
            </a>
           </li>
           <li>
            <a href="/wiki/Forecast_bias" title="Forecast bias">
             Forecast
            </a>
           </li>
           <li>
            <a href="/wiki/Healthy_user_bias" title="Healthy user bias">
             Healthy user
            </a>
           </li>
           <li>
            <a href="/wiki/Information_bias_(epidemiology)" title="Information bias (epidemiology)">
             Information
            </a>
            <ul>
             <li>
              <a href="/wiki/Information_bias_(psychology)" title="Information bias (psychology)">
               Psychological
              </a>
             </li>
            </ul>
           </li>
           <li>
            <a href="/wiki/Lead_time_bias" title="Lead time bias">
             Lead time
            </a>
           </li>
           <li>
            <a href="/wiki/Length_time_bias" title="Length time bias">
             Length time
            </a>
           </li>
           <li>
            <a href="/wiki/Participation_bias" title="Participation bias">
             Non-response
            </a>
           </li>
           <li>
            <a href="/wiki/Observer_bias" title="Observer bias">
             Observer
            </a>
           </li>
           <li>
            <a href="/wiki/Omitted-variable_bias" title="Omitted-variable bias">
             Omitted-variable
            </a>
           </li>
           <li>
            <a href="/wiki/Participation_bias" title="Participation bias">
             Participation
            </a>
           </li>
           <li>
            <a href="/wiki/Recall_bias" title="Recall bias">
             Recall
            </a>
           </li>
           <li>
            <a href="/wiki/Sampling_bias" title="Sampling bias">
             Sampling
            </a>
           </li>
           <li>
            <a href="/wiki/Selection_bias" title="Selection bias">
             Selection
            </a>
           </li>
           <li>
            <a href="/wiki/Self-selection_bias" title="Self-selection bias">
             Self-selection
            </a>
           </li>
           <li>
            <a href="/wiki/Social-desirability_bias" title="Social-desirability bias">
             Social desirability
            </a>
           </li>
           <li>
            <a href="/wiki/Spectrum_bias" title="Spectrum bias">
             Spectrum
            </a>
           </li>
           <li>
            <a href="/wiki/Survivorship_bias" title="Survivorship bias">
             Survivorship
            </a>
           </li>
           <li>
            <a class="mw-redirect" href="/wiki/Systematic_error" title="Systematic error">
             Systematic error
            </a>
           </li>
           <li>
            <a href="/wiki/Systemic_bias" title="Systemic bias">
             Systemic
            </a>
           </li>
           <li>
            <a href="/wiki/Verification_bias" title="Verification bias">
             Verification
            </a>
           </li>
           <li>
            <a href="/wiki/Wet_bias" title="Wet bias">
             Wet
            </a>
           </li>
          </ul>
         </div>
        </td>
       </tr>
       <tr>
        <th class="navbox-group" scope="row" style="width:1%">
         Other biases
        </th>
        <td class="navbox-list-with-group navbox-list navbox-odd hlist" style="width:100%;padding:0">
         <div style="padding:0 0.25em">
          <ul>
           <li>
            <a href="/wiki/Academic_bias" title="Academic bias">
             Academic
            </a>
           </li>
           <li>
            <a href="/wiki/Basking_in_reflected_glory" title="Basking in reflected glory">
             Basking in reflected glory
            </a>
           </li>
           <li>
            <i>
             <a href="/wiki/D%C3%A9formation_professionnelle" title="Déformation professionnelle">
              Déformation professionnelle
             </a>
            </i>
           </li>
           <li>
            <a href="/wiki/Funding_bias" title="Funding bias">
             Funding
            </a>
           </li>
           <li>
            <a class="mw-redirect" href="/wiki/FUTON_bias" title="FUTON bias">
             FUTON
            </a>
           </li>
           <li>
            <a href="/wiki/Inductive_bias" title="Inductive bias">
             Inductive
            </a>
           </li>
           <li>
            <a href="/wiki/Infrastructure_bias" title="Infrastructure bias">
             Infrastructure
            </a>
           </li>
           <li>
            <a href="/wiki/Inherent_bias" title="Inherent bias">
             Inherent
            </a>
           </li>
           <li>
            <a class="mw-redirect" href="/wiki/Bias_in_education" title="Bias in education">
             In education
            </a>
           </li>
           <li>
            <a href="/wiki/Liking_gap" title="Liking gap">
             Liking gap
            </a>
           </li>
           <li>
            <a href="/wiki/Media_bias" title="Media bias">
             Media
            </a>
            <ul>
             <li>
              <a href="/wiki/False_balance" title="False balance">
               False balance
              </a>
             </li>
             <li>
              <a href="/wiki/United_States_news_media_and_the_Vietnam_War" title="United States news media and the Vietnam War">
               Vietnam War
              </a>
             </li>
             <li>
              <a class="mw-redirect" href="/wiki/Media_of_Norway" title="Media of Norway">
               Norway
              </a>
             </li>
             <li>
              <a href="/wiki/Media_bias_in_South_Asia" title="Media bias in South Asia">
               South Asia
              </a>
             </li>
             <li>
              <a class="mw-redirect" href="/wiki/Media_of_Sweden" title="Media of Sweden">
               Sweden
              </a>
             </li>
             <li>
              <a href="/wiki/Media_bias_in_the_United_States" title="Media bias in the United States">
               United States
              </a>
             </li>
             <li>
              <a class="mw-redirect" href="/wiki/Media_coverage_of_the_Arab%E2%80%93Israeli_conflict" title="Media coverage of the Arab–Israeli conflict">
               Arab–Israeli conflict
              </a>
             </li>
             <li>
              <a class="mw-redirect" href="/wiki/Media_portrayal_of_the_Ukrainian_crisis" title="Media portrayal of the Ukrainian crisis">
               Ukraine
              </a>
             </li>
            </ul>
           </li>
           <li>
            <a href="/wiki/Net_bias" title="Net bias">
             Net
            </a>
           </li>
           <li>
            <a href="/wiki/Political_bias" title="Political bias">
             Political bias
            </a>
           </li>
           <li>
            <a href="/wiki/Publication_bias" title="Publication bias">
             Publication
            </a>
           </li>
           <li>
            <a href="/wiki/Reporting_bias" title="Reporting bias">
             Reporting
            </a>
           </li>
           <li>
            <a href="/wiki/White_hat_bias" title="White hat bias">
             White hat
            </a>
           </li>
          </ul>
         </div>
        </td>
       </tr>
       <tr>
        <th class="navbox-group" scope="row" style="width:1%">
         Bias reduction
        </th>
        <td class="navbox-list-with-group navbox-list navbox-even hlist" style="width:100%;padding:0">
         <div style="padding:0 0.25em">
          <ul>
           <li>
            <a href="/wiki/Cognitive_bias_mitigation" title="Cognitive bias mitigation">
             Cognitive bias mitigation
            </a>
           </li>
           <li>
            <a href="/wiki/Debiasing" title="Debiasing">
             Debiasing
            </a>
           </li>
           <li>
            <a href="/wiki/Heuristic_(psychology)" title="Heuristic (psychology)">
             Heuristics in judgment and decision-making
            </a>
           </li>
          </ul>
         </div>
        </td>
       </tr>
       <tr>
        <td class="navbox-abovebelow hlist" colspan="2">
         <div>
          <ul>
           <li>
            Lists:
            <a href="/wiki/List_of_cognitive_biases" title="List of cognitive biases">
             General
            </a>
           </li>
           <li>
            <a href="/wiki/List_of_cognitive_biases#Memory_biases" title="List of cognitive biases">
             Memory
            </a>
           </li>
          </ul>
         </div>
        </td>
       </tr>
      </tbody>
     </table>
    </div>
    <!-- 
NewPP limit report
Parsed by mw‐web.eqiad.main‐7f58d5dcf5‐9zzw6
Cached time: 20241110213955
Cache expiry: 2592000
Reduced expiry: false
Complications: [vary‐revision‐sha1, show‐toc]
CPU time usage: 0.457 seconds
Real time usage: 0.567 seconds
Preprocessor visited node count: 2163/1000000
Post‐expand include size: 84708/2097152 bytes
Template argument size: 4255/2097152 bytes
Highest expansion depth: 12/100
Expensive parser function count: 2/500
Unstrip recursion depth: 1/20
Unstrip post‐expand size: 123924/5000000 bytes
Lua time usage: 0.256/10.000 seconds
Lua memory usage: 6298260/52428800 bytes
Number of Wikibase entities loaded: 0/400
-->
    <!--
Transclusion expansion time report (%,ms,calls,template)
100.00%  480.886      1 -total
 44.37%  213.384      1 Template:Reflist
 22.65%  108.943      4 Template:Cite_book
 18.53%   89.096      1 Template:Automation
 17.81%   85.658      1 Template:Sidebar
 15.72%   75.579     15 Template:Cite_journal
 13.21%   63.528      1 Template:Short_description
  8.53%   41.001      1 Template:Vague
  7.97%   38.318      1 Template:Biases
  7.85%   37.743      2 Template:Pagetype
-->
    <!-- Saved in parser cache with key enwiki:pcache:idhash:41145357-0!canonical and timestamp 20241110213955 and revision id 1217953064. Rendering was triggered because: page-view
 -->
   </meta>
  </div>
  <!--esi <esi:include src="/esitest-fa8a495983347898/content" /> -->
  <noscript>
   <img alt="" height="1" src="https://login.wikimedia.org/wiki/Special:CentralAutoLogin/start?type=1x1" style="border: none; position: absolute;" width="1"/>
  </noscript>
  <div class="printfooter" data-nosnippet="">
   Retrieved from "
   <a dir="ltr" href="https://en.wikipedia.org/w/index.php?title=Automation_bias&amp;oldid=1217953064">
    https://en.wikipedia.org/w/index.php?title=Automation_bias&amp;oldid=1217953064
   </a>
   "
  </div>
 </div>
 <div class="catlinks" data-mw="interface" id="catlinks">
  <div class="mw-normal-catlinks" id="mw-normal-catlinks">
   <a href="/wiki/Help:Category" title="Help:Category">
    Categories
   </a>
   :
   <ul>
    <li>
     <a href="/wiki/Category:Cognitive_biases" title="Category:Cognitive biases">
      Cognitive biases
     </a>
    </li>
    <li>
     <a href="/wiki/Category:Impact_of_automation" title="Category:Impact of automation">
      Impact of automation
     </a>
    </li>
    <li>
     <a href="/wiki/Category:Prospect_theory" title="Category:Prospect theory">
      Prospect theory
     </a>
    </li>
   </ul>
  </div>
  <div class="mw-hidden-catlinks mw-hidden-cats-hidden" id="mw-hidden-catlinks">
   Hidden categories:
   <ul>
    <li>
     <a href="/wiki/Category:CS1_maint:_bot:_original_URL_status_unknown" title="Category:CS1 maint: bot: original URL status unknown">
      CS1 maint: bot: original URL status unknown
     </a>
    </li>
    <li>
     <a href="/wiki/Category:Articles_with_short_description" title="Category:Articles with short description">
      Articles with short description
     </a>
    </li>
    <li>
     <a href="/wiki/Category:Short_description_is_different_from_Wikidata" title="Category:Short description is different from Wikidata">
      Short description is different from Wikidata
     </a>
    </li>
    <li>
     <a href="/wiki/Category:All_Wikipedia_articles_needing_clarification" title="Category:All Wikipedia articles needing clarification">
      All Wikipedia articles needing clarification
     </a>
    </li>
    <li>
     <a href="/wiki/Category:Wikipedia_articles_needing_clarification_from_December_2019" title="Category:Wikipedia articles needing clarification from December 2019">
      Wikipedia articles needing clarification from December 2019
     </a>
    </li>
   </ul>
  </div>
 </div>
</div>
